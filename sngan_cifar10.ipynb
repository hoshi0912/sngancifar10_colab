{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sngan_cifar10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoshi0912/sngancifar10_colab/blob/master/sngan_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LievEKFXPf2f",
        "colab_type": "code",
        "outputId": "de092c80-67b4-464d-e1f4-9ba7c661ea33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy\n",
        "import numpy as np\n",
        "\n",
        "import chainer\n",
        "from chainer import cuda\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "from chainer import Variable\n",
        "from chainer.training import extensions\n",
        "from chainer import initializers\n",
        "from chainer import link\n",
        "from chainer.utils import argument\n",
        "from chainer import variable\n",
        "from chainer.links import EmbedID\n",
        "from chainer import configuration\n",
        "from chainer.functions.normalization import batch_normalization\n",
        "from chainer.functions.array.broadcast import broadcast_to\n",
        "from chainer.initializers import normal\n",
        "from chainer.functions.connection import embed_id\n",
        "from chainer.functions.connection import linear\n",
        "from chainer.links.connection.linear import Linear\n",
        "from chainer.functions.connection import convolution_2d\n",
        "from chainer.links.connection.convolution_2d import Convolution2D\n",
        "import math\n",
        "\n",
        "\n",
        "chainer.print_runtime_info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Platform: Linux-4.14.79+-x86_64-with-Ubuntu-18.04-bionic\n",
            "Chainer: 5.4.0\n",
            "NumPy: 1.16.3\n",
            "CuPy:\n",
            "  CuPy Version          : 5.4.0\n",
            "  CUDA Root             : /usr/local/cuda\n",
            "  CUDA Build Version    : 10000\n",
            "  CUDA Driver Version   : 10000\n",
            "  CUDA Runtime Version  : 10000\n",
            "  cuDNN Build Version   : 7301\n",
            "  cuDNN Version         : 7301\n",
            "  NCCL Build Version    : 2402\n",
            "  NCCL Runtime Version  : 2402\n",
            "iDeep: 2.0.0.post3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzypAWA4QDXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parameters\n",
        "n_epoch = 100  # number of epochs\n",
        "n_hidden = 128  # number of hidden units\n",
        "batchsize = 64  # minibatch size\n",
        "snapshot_interval = 1000  # number of iterations per snapshots\n",
        "display_interval = 100  # number of iterations per display the status\n",
        "gpu_id = 0\n",
        "out_dir = 'result'\n",
        "seed = 0  # random seed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vim3C3aaZbbH",
        "colab_type": "code",
        "outputId": "2f525b9a-5e51-4fa2-9b40-74ced0a0b7b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load the CIFAR10 dataset if args.dataset is not specified\n",
        "train, _ = chainer.datasets.get_cifar10(withlabel=True, ndim=3, scale=255.)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRElu1xrc4XU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter = chainer.iterators.SerialIterator(train, batchsize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v7bvG9gGkSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SNLinear(Linear):\n",
        "    \"\"\"Linear layer with Spectral Normalization.\n",
        "    Args:\n",
        "        in_size (int): Dimension of input vectors. If ``None``, parameter\n",
        "            initialization will be deferred until the first forward datasets pass\n",
        "            at which time the size will be determined.\n",
        "        out_size (int): Dimension of output vectors.\n",
        "        wscale (float): Scaling factor of the weight matrix.\n",
        "        bias (float): Initial bias value.\n",
        "        nobias (bool): If ``True``, then this function does not use the bias.\n",
        "        initialW (2-D array): Initial weight value. If ``None``, then this\n",
        "            function uses to initialize ``wscale``.\n",
        "            May also be a callable that takes ``numpy.ndarray`` or\n",
        "            ``cupy.ndarray`` and edits its value.\n",
        "        initial_bias (1-D array): Initial bias value. If ``None``, then this\n",
        "            function uses to initialize ``bias``.\n",
        "            May also be a callable that takes ``numpy.ndarray`` or\n",
        "            ``cupy.ndarray`` and edits its value.\n",
        "        use_gamma (bool): If true, apply scalar multiplication to the \n",
        "            normalized weight (i.e. reparameterize).\n",
        "        Ip (int): The number of power iteration for calculating the spcetral \n",
        "            norm of the weights.\n",
        "        factor (float) : constant factor to adjust spectral norm of W_bar.\n",
        "    .. seealso:: :func:`~chainer.functions.linear`\n",
        "    Attributes:\n",
        "        W (~chainer.Variable): Weight parameter.\n",
        "        W_bar (~chainer.Variable): Spectrally normalized weight parameter.\n",
        "        b (~chainer.Variable): Bias parameter.\n",
        "        u (~numpy.array): Current estimation of the right largest singular vector of W.\n",
        "        (optional) gamma (~chainer.Variable): the multiplier parameter.\n",
        "        (optional) factor (float): constant factor to adjust spectral norm of W_bar.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_size, out_size, use_gamma=False, nobias=False,\n",
        "                 initialW=None, initial_bias=None, Ip=1, factor=None):\n",
        "        self.Ip = Ip\n",
        "        self.use_gamma = use_gamma\n",
        "        self.factor = factor\n",
        "        super(SNLinear, self).__init__(\n",
        "            in_size, out_size, nobias, initialW, initial_bias\n",
        "        )\n",
        "        self.u = np.random.normal(size=(1, out_size)).astype(dtype=\"f\")\n",
        "        self.register_persistent('u')\n",
        "\n",
        "    @property\n",
        "    def W_bar(self):\n",
        "        \"\"\"\n",
        "        Spectral Normalized Weight\n",
        "        \"\"\"\n",
        "        sigma, _u, _ = max_singular_value(self.W, self.u, self.Ip)\n",
        "        if self.factor:\n",
        "            sigma = sigma / self.factor\n",
        "        sigma = broadcast_to(sigma.reshape((1, 1)), self.W.shape)\n",
        "        self.u[:] = _u\n",
        "        if hasattr(self, 'gamma'):\n",
        "            return broadcast_to(self.gamma, self.W.shape) * self.W / sigma\n",
        "        else:\n",
        "            return self.W / sigma\n",
        "\n",
        "    def _initialize_params(self, in_size):\n",
        "        super(SNLinear, self)._initialize_params(in_size)\n",
        "        if self.use_gamma:\n",
        "            _, s, _ = np.linalg.svd(self.W.data)\n",
        "            with self.init_scope():\n",
        "                self.gamma = chainer.Parameter(s[0], (1, 1))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Applies the linear layer.\n",
        "        Args:\n",
        "            x (~chainer.Variable): Batch of input vectors.\n",
        "        Returns:\n",
        "            ~chainer.Variable: Output of the linear layer.\n",
        "        \"\"\"\n",
        "        if self.W.data is None:\n",
        "            self._initialize_params(x.size // x.shape[0])\n",
        "        return linear.linear(x, self.W_bar, self.b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK9BjBhbVJ5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConditionalBatchNormalization(chainer.Chain):\n",
        "    \"\"\"\n",
        "    Conditional Batch Normalization\n",
        "    Args:\n",
        "        size (int or tuple of ints): Size (or shape) of channel\n",
        "            dimensions.\n",
        "        n_cat (int): the number of categories of categorical variable.\n",
        "        decay (float): Decay rate of moving average. It is used on training.\n",
        "        eps (float): Epsilon value for numerical stability.\n",
        "        dtype (numpy.dtype): Type to use in computing.\n",
        "        use_gamma (bool): If ``True``, use scaling parameter. Otherwise, use\n",
        "            unit(1) which makes no effect.\n",
        "        use_beta (bool): If ``True``, use shifting parameter. Otherwise, use\n",
        "            unit(0) which makes no effect.\n",
        "    See: `Batch Normalization: Accelerating Deep Network Training by Reducing\\\n",
        "          Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`_\n",
        "    .. seealso::\n",
        "       :func:`~chainer.functions.batch_normalization`,\n",
        "       :func:`~chainer.functions.fixed_batch_normalization`\n",
        "    Attributes:\n",
        "        gamma (~chainer.Variable): Scaling parameter.\n",
        "        beta (~chainer.Variable): Shifting parameter.\n",
        "        avg_mean (numpy.ndarray or cupy.ndarray): Population mean.\n",
        "        avg_var (numpy.ndarray or cupy.ndarray): Population variance.\n",
        "        N (int): Count of batches given for fine-tuning.\n",
        "        decay (float): Decay rate of moving average. It is used on training.\n",
        "        eps (float): Epsilon value for numerical stability. This value is added\n",
        "            to the batch variances.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, n_cat, decay=0.9, eps=2e-5, dtype=numpy.float32):\n",
        "        super(ConditionalBatchNormalization, self).__init__()\n",
        "        self.avg_mean = numpy.zeros(size, dtype=dtype)\n",
        "        self.register_persistent('avg_mean')\n",
        "        self.avg_var = numpy.zeros(size, dtype=dtype)\n",
        "        self.register_persistent('avg_var')\n",
        "        self.N = 0\n",
        "        self.register_persistent('N')\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "        self.n_cat = n_cat\n",
        "\n",
        "    def __call__(self, x, gamma, beta, **kwargs):\n",
        "        \"\"\"__call__(self, x, c, finetune=False)\n",
        "        Invokes the forward propagation of BatchNormalization.\n",
        "        In training mode, the BatchNormalization computes moving averages of\n",
        "        mean and variance for evaluatino during training, and normalizes the\n",
        "        input using batch statistics.\n",
        "        .. warning::\n",
        "           ``test`` argument is not supported anymore since v2.\n",
        "           Instead, use ``chainer.using_config('train', train)``.\n",
        "           See :func:`chainer.using_config`.\n",
        "        Args:\n",
        "            x (Variable): Input variable.\n",
        "            gamma (Variable): Input variable of gamma of shape\n",
        "            finetune (bool): If it is in the training mode and ``finetune`` is\n",
        "                ``True``, BatchNormalization runs in fine-tuning mode; it\n",
        "                accumulates the input array to compute population statistics\n",
        "                for normalization, and normalizes the input using batch\n",
        "                statistics.\n",
        "        \"\"\"\n",
        "        argument.check_unexpected_kwargs(\n",
        "            kwargs, test='test argument is not supported anymore. '\n",
        "                         'Use chainer.using_config')\n",
        "        finetune, = argument.parse_kwargs(kwargs, ('finetune', False))\n",
        "        with cuda.get_device_from_id(self._device_id):\n",
        "            _gamma = variable.Variable(self.xp.ones(\n",
        "                self.avg_mean.shape, dtype=x.dtype))\n",
        "        with cuda.get_device_from_id(self._device_id):\n",
        "            _beta = variable.Variable(self.xp.zeros(\n",
        "                self.avg_mean.shape, dtype=x.dtype))\n",
        "        if configuration.config.train:\n",
        "            if finetune:\n",
        "                self.N += 1\n",
        "                decay = 1. - 1. / self.N\n",
        "            else:\n",
        "                decay = self.decay\n",
        "            ret = chainer.functions.batch_normalization(x, _gamma, _beta, eps=self.eps, running_mean=self.avg_mean,\n",
        "                                                        running_var=self.avg_var, decay=decay)\n",
        "        else:\n",
        "            # Use running average statistics or fine-tuned statistics.\n",
        "            mean = variable.Variable(self.avg_mean)\n",
        "            var = variable.Variable(self.avg_var)\n",
        "            ret = batch_normalization.fixed_batch_normalization(\n",
        "                x, _gamma, _beta, mean, var, self.eps)\n",
        "        shape = ret.shape\n",
        "        ndim = len(shape)\n",
        "        gamma = F.broadcast_to(F.reshape(gamma, list(gamma.shape) + [1] * (ndim - len(gamma.shape))), shape)\n",
        "        beta = F.broadcast_to(F.reshape(beta, list(beta.shape) + [1] * (ndim - len(beta.shape))), shape)\n",
        "        return gamma * ret + beta\n",
        "\n",
        "\n",
        "def start_finetuning(self):\n",
        "    \"\"\"Resets the population count for collecting population statistics.\n",
        "    This method can be skipped if it is the first time to use the\n",
        "    fine-tuning mode. Otherwise, this method should be called before\n",
        "    starting the fine-tuning mode again.\n",
        "    \"\"\"\n",
        "    self.N = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMgctWoUU5_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CategoricalConditionalBatchNormalization(ConditionalBatchNormalization):\n",
        "    \"\"\"\n",
        "    Conditional Batch Normalization\n",
        "    Args:\n",
        "        size (int or tuple of ints): Size (or shape) of channel\n",
        "            dimensions.\n",
        "        n_cat (int): the number of categories of categorical variable.\n",
        "        decay (float): Decay rate of moving average. It is used on training.\n",
        "        eps (float): Epsilon value for numerical stability.\n",
        "        dtype (numpy.dtype): Type to use in computing.\n",
        "        use_gamma (bool): If ``True``, use scaling parameter. Otherwise, use\n",
        "            unit(1) which makes no effect.\n",
        "        use_beta (bool): If ``True``, use shifting parameter. Otherwise, use\n",
        "            unit(0) which makes no effect.\n",
        "    See: `Batch Normalization: Accelerating Deep Network Training by Reducing\\\n",
        "          Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`_\n",
        "    .. seealso::\n",
        "       :func:`~chainer.functions.batch_normalization`,\n",
        "       :func:`~chainer.functions.fixed_batch_normalization`\n",
        "    Attributes:\n",
        "        gamma (~chainer.Variable): Scaling parameter.\n",
        "        beta (~chainer.Variable): Shifting parameter.\n",
        "        avg_mean (numpy.ndarray or cupy.ndarray): Population mean.\n",
        "        avg_var (numpy.ndarray or cupy.ndarray): Population variance.\n",
        "        N (int): Count of batches given for fine-tuning.\n",
        "        decay (float): Decay rate of moving average. It is used on training.\n",
        "        eps (float): Epsilon value for numerical stability. This value is added\n",
        "            to the batch variances.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, n_cat, decay=0.9, eps=2e-5, dtype=numpy.float32,\n",
        "                 initial_gamma=None, initial_beta=None):\n",
        "        super(CategoricalConditionalBatchNormalization, self).__init__(\n",
        "            size=size, n_cat=n_cat, decay=decay, eps=eps, dtype=dtype)\n",
        "\n",
        "        with self.init_scope():\n",
        "            if initial_gamma is None:\n",
        "                initial_gamma = 1\n",
        "            initial_gamma = initializers._get_initializer(initial_gamma)\n",
        "            initial_gamma.dtype = dtype\n",
        "            self.gammas = EmbedID(n_cat, size, initialW=initial_gamma)\n",
        "            if initial_beta is None:\n",
        "                initial_beta = 0\n",
        "            initial_beta = initializers._get_initializer(initial_beta)\n",
        "            initial_beta.dtype = dtype\n",
        "            self.betas = EmbedID(n_cat, size, initialW=initial_beta)\n",
        "\n",
        "    def __call__(self, x, c, finetune=False, **kwargs):\n",
        "        \"\"\"__call__(self, x, c, finetune=False)\n",
        "        Invokes the forward propagation of BatchNormalization.\n",
        "        In training mode, the BatchNormalization computes moving averages of\n",
        "        mean and variance for evaluatino during training, and normalizes the\n",
        "        input using batch statistics.\n",
        "        .. warning::\n",
        "           ``test`` argument is not supported anymore since v2.\n",
        "           Instead, use ``chainer.using_config('train', train)``.\n",
        "           See :func:`chainer.using_config`.\n",
        "        Args:\n",
        "            x (Variable): Input variable.\n",
        "            c (Variable): Input variable for conditioning gamma and beta\n",
        "            finetune (bool): If it is in the training mode and ``finetune`` is\n",
        "                ``True``, BatchNormalization runs in fine-tuning mode; it\n",
        "                accumulates the input array to compute population statistics\n",
        "                for normalization, and normalizes the input using batch\n",
        "                statistics.\n",
        "        \"\"\"\n",
        "        weights, = argument.parse_kwargs(kwargs, ('weights', None))\n",
        "        if c.ndim == 2 and weights is not None:\n",
        "            _gamma_c = self.gammas(c)\n",
        "            _beta_c = self.betas(c)\n",
        "            _gamma_c = F.broadcast_to(F.expand_dims(weights, 2), _gamma_c.shape) * _gamma_c \n",
        "            _beta_c = F.broadcast_to(F.expand_dims(weights, 2), _beta_c.shape) * _beta_c\n",
        "            gamma_c = F.sum(_gamma_c, 1) \n",
        "            beta_c = F.sum(_beta_c, 1)\n",
        "        else:\n",
        "            gamma_c = self.gammas(c)\n",
        "            beta_c = self.betas(c)\n",
        "        return super(CategoricalConditionalBatchNormalization, self).__call__(x, gamma_c, beta_c, **kwargs)\n",
        "\n",
        "\n",
        "def start_finetuning(self):\n",
        "    \"\"\"Resets the population count for collecting population statistics.\n",
        "    This method can be skipped if it is the first time to use the\n",
        "    fine-tuning mode. Otherwise, this method should be called before\n",
        "    starting the fine-tuning mode again.\n",
        "    \"\"\"\n",
        "    self.N = 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iZSGpenRcne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _upsample(x):\n",
        "    h, w = x.shape[2:]\n",
        "    return F.unpooling_2d(x, 2, outsize=(h * 2, w * 2))\n",
        "\n",
        "\n",
        "def upsample_conv(x, conv):\n",
        "    return conv(_upsample(x))\n",
        "\n",
        "\n",
        "class GBlock(chainer.Chain):\n",
        "    def __init__(self, in_channels, out_channels, hidden_channels=None, ksize=3, pad=1,\n",
        "                 activation=F.relu, upsample=False, n_classes=0):\n",
        "        super(GBlock, self).__init__()\n",
        "        initializer = chainer.initializers.GlorotUniform(math.sqrt(2))\n",
        "        initializer_sc = chainer.initializers.GlorotUniform()\n",
        "        self.activation = activation\n",
        "        self.upsample = upsample\n",
        "        self.learnable_sc = in_channels != out_channels or upsample\n",
        "        hidden_channels = out_channels if hidden_channels is None else hidden_channels\n",
        "        self.n_classes = n_classes\n",
        "        with self.init_scope():\n",
        "            self.c1 = L.Convolution2D(in_channels, hidden_channels, ksize=ksize, pad=pad, initialW=initializer)\n",
        "            self.c2 = L.Convolution2D(hidden_channels, out_channels, ksize=ksize, pad=pad, initialW=initializer)\n",
        "            if n_classes > 0:\n",
        "                self.b1 = CategoricalConditionalBatchNormalization(in_channels, n_cat=n_classes)\n",
        "                self.b2 = CategoricalConditionalBatchNormalization(hidden_channels, n_cat=n_classes)\n",
        "            else:\n",
        "                self.b1 = L.BatchNormalization(in_channels)\n",
        "                self.b2 = L.BatchNormalization(hidden_channels)\n",
        "            if self.learnable_sc:\n",
        "                self.c_sc = L.Convolution2D(in_channels, out_channels, ksize=1, pad=0, initialW=initializer_sc)\n",
        "\n",
        "    def residual(self, x, y=None, z=None, **kwargs):\n",
        "        h = x\n",
        "        h = self.b1(h, y, **kwargs) if y is not None else self.b1(h, **kwargs)\n",
        "        h = self.activation(h)\n",
        "        h = upsample_conv(h, self.c1) if self.upsample else self.c1(h)\n",
        "        h = self.b2(h, y, **kwargs) if y is not None else self.b2(h, **kwargs)\n",
        "        h = self.activation(h)\n",
        "        h = self.c2(h)\n",
        "        return h\n",
        "\n",
        "    def shortcut(self, x):\n",
        "        if self.learnable_sc:\n",
        "            x = upsample_conv(x, self.c_sc) if self.upsample else self.c_sc(x)\n",
        "            return x\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def __call__(self, x, y=None, z=None, **kwargs):\n",
        "        return self.residual(x, y, z, **kwargs) + self.shortcut(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBi239mYQCjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(chainer.Chain):\n",
        "\n",
        "    def __init__(self, n_hidden = 128, n_classes=10, bottom_width=4, bottom_height=4, ch=256, wscale=0.02, activation=F.relu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.n_hidden = n_hidden\n",
        "        self.ch = ch\n",
        "        self.bottom_width = bottom_width\n",
        "        self.bottom_height = bottom_height\n",
        "        self.activation = activation\n",
        "\n",
        "        with self.init_scope():\n",
        "            w = chainer.initializers.GlorotUniform()\n",
        "            self.l1 = L.Linear(n_hidden, (bottom_width ** 2) * ch, initialW=w)\n",
        "            self.block2 = GBlock(ch, ch, activation=activation, upsample=True, n_classes=n_classes)\n",
        "            self.block3 = GBlock(ch, ch, activation=activation, upsample=True, n_classes=n_classes)\n",
        "            self.block4 = GBlock(ch, ch, activation=activation, upsample=True, n_classes=n_classes)\n",
        "            self.block5 = GBlock(ch * 4, ch * 2, activation=activation, upsample=True, n_classes=n_classes)\n",
        "            self.block6 = GBlock(ch * 2, ch, activation=activation, upsample=True, n_classes=n_classes)\n",
        "            self.b7 = L.BatchNormalization(ch)\n",
        "            self.l7 = L.Convolution2D(ch, 3, ksize=3, stride=1, pad=1, initialW=w)\n",
        "\n",
        "    def make_hidden(self, batchsize):\n",
        "        #np.random.uniform(-1, 1, (batchsize, self.n_hidden, 1, 1)).astype(np.float32)\n",
        "        return np.random.randn(batchsize, n_hidden).astype(np.float32)\n",
        "\n",
        "    def __call__(self, z, c, **kwargs):\n",
        "        #h = F.reshape(F.relu(self.bn0(self.l0(z))),(len(z), -1, self.bottom_width, self.bottom_height))\n",
        "        h = self.l1(z)\n",
        "        h = F.reshape(h,(h.shape[0], -1, self.bottom_width, self.bottom_height))\n",
        "        h = self.block2(h, c, **kwargs)\n",
        "        h = self.block3(h, c, **kwargs)\n",
        "        h = self.block4(h, c, **kwargs)\n",
        "        #h = self.block5(h, c)\n",
        "        #h = self.block6(h, c)\n",
        "        h = self.b7(h)\n",
        "        h = self.activation(h)\n",
        "        h = F.tanh(self.l7(h))\n",
        "        return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRWiOnzYpeg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _l2normalize(v, eps=1e-12):\n",
        "    norm = cuda.reduce('T x', 'T out',\n",
        "                       'x * x', 'a + b', 'out = sqrt(a)', 0,\n",
        "                       'norm_sn')\n",
        "    div = cuda.elementwise('T x, T norm, T eps',\n",
        "                           'T out',\n",
        "                           'out = x / (norm + eps)',\n",
        "                           'div_sn')\n",
        "    return div(v, norm(v), eps)\n",
        "\n",
        "\n",
        "def max_singular_value(W, u=None, Ip=1):\n",
        "    \"\"\"\n",
        "    Apply power iteration for the weight parameter\n",
        "    \"\"\"\n",
        "    if not Ip >= 1:\n",
        "        raise ValueError(\"The number of power iterations should be positive integer\")\n",
        "\n",
        "    xp = cuda.get_array_module(W.data)\n",
        "    if u is None:\n",
        "        u = xp.random.normal(size=(1, W.shape[0])).astype(xp.float32)\n",
        "    _u = u\n",
        "    for _ in range(Ip):\n",
        "        _v = _l2normalize(xp.dot(_u, W.data), eps=1e-12)\n",
        "        _u = _l2normalize(xp.dot(_v, W.data.transpose()), eps=1e-12)\n",
        "    sigma = F.sum(F.linear(_u, F.transpose(W)) * _v)\n",
        "    return sigma, _u, _v\n",
        "\n",
        "\n",
        "def max_singular_value_fully_differentiable(W, u=None, Ip=1):\n",
        "    \"\"\"\n",
        "    Apply power iteration for the weight parameter (fully differentiable version)\n",
        "    \"\"\"\n",
        "    if not Ip >= 1:\n",
        "        raise ValueError(\"The number of power iterations should be positive integer\")\n",
        "\n",
        "    xp = cuda.get_array_module(W.data)\n",
        "    if u is None:\n",
        "        u = xp.random.normal(size=(1, W.shape[0])).astype(xp.float32)\n",
        "    _u = u\n",
        "    for _ in range(Ip):\n",
        "        _v = F.normalize(F.matmul(_u, W), eps=1e-12)\n",
        "        _u = F.normalize(F.matmul(_v, F.transpose(W)), eps=1e-12)\n",
        "    _u = F.matmul(_v, F.transpose(W))\n",
        "    norm = F.sqrt(F.sum(_u ** 2))\n",
        "    return norm, _l2normalize(_u.data), _v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Amo53HLpLZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SNEmbedID(link.Link):\n",
        "    \"\"\"Efficient linear layer for one-hot input.\n",
        "    This is a link that wraps the :func:`~chainer.functions.embed_id` function.\n",
        "    This link holds the ID (word) embedding matrix ``W`` as a parameter.\n",
        "    Args:\n",
        "        in_size (int): Number of different identifiers (a.k.a. vocabulary\n",
        "            size).\n",
        "        out_size (int): Size of embedding vector.\n",
        "        initialW (2-D array): Initial weight value. If ``None``, then the\n",
        "            matrix is initialized from the standard normal distribution.\n",
        "            May also be a callable that takes ``numpy.ndarray`` or\n",
        "            ``cupy.ndarray`` and edits its value.\n",
        "        ignore_label (int or None): If ``ignore_label`` is an int value,\n",
        "            ``i``-th column of return value is filled with ``0``.\n",
        "        Ip (int): The number of power iteration for calculating the spcetral\n",
        "            norm of the weights.\n",
        "        factor (float) : constant factor to adjust spectral norm of W_bar.\n",
        "    .. seealso:: :func:`chainer.functions.embed_id`\n",
        "    Attributes:\n",
        "        W (~chainer.Variable): Embedding parameter matrix.\n",
        "        W_bar (~chainer.Variable): Spectrally normalized weight parameter.\n",
        "        u (~numpy.array): Current estimation of the right largest singular vector of W.\n",
        "        (optional) gamma (~chainer.Variable): the multiplier parameter.\n",
        "        (optional) factor (float): constant factor to adjust spectral norm of W_bar.\n",
        "    \"\"\"\n",
        "\n",
        "    ignore_label = None\n",
        "\n",
        "    def __init__(self, in_size, out_size, initialW=None, ignore_label=None, Ip=1, factor=None):\n",
        "        super(SNEmbedID, self).__init__()\n",
        "        self.ignore_label = ignore_label\n",
        "        self.Ip = Ip\n",
        "        self.factor = factor\n",
        "        with self.init_scope():\n",
        "            if initialW is None:\n",
        "                initialW = normal.Normal(1.0)\n",
        "            self.W = variable.Parameter(initialW, (in_size, out_size))\n",
        "\n",
        "        self.u = np.random.normal(size=(1, in_size)).astype(dtype=\"f\")\n",
        "        self.register_persistent('u')\n",
        "\n",
        "    @property\n",
        "    def W_bar(self):\n",
        "        \"\"\"\n",
        "        Spectral Normalized Weight\n",
        "        \"\"\"\n",
        "        sigma, _u, _ = max_singular_value(self.W, self.u, self.Ip)\n",
        "        if self.factor:\n",
        "            sigma = sigma / self.factor\n",
        "        sigma = broadcast_to(sigma.reshape((1, 1)), self.W.shape)\n",
        "        self.u[:] = _u\n",
        "        return self.W / sigma\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Extracts the word embedding of given IDs.\n",
        "        Args:\n",
        "            x (~chainer.Variable): Batch vectors of IDs.\n",
        "        Returns:\n",
        "            ~chainer.Variable: Batch of corresponding embeddings.\n",
        "        \"\"\"\n",
        "        return embed_id.embed_id(x, self.W_bar, ignore_label=self.ignore_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ykqsePhTOBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SNConvolution2D(Convolution2D):\n",
        "    \"\"\"Two-dimensional convolutional layer with spectral normalization.\n",
        "    This link wraps the :func:`~chainer.functions.convolution_2d` function and\n",
        "    holds the filter weight and bias vector as parameters.\n",
        "    Args:\n",
        "        in_channels (int): Number of channels of input arrays. If ``None``,\n",
        "            parameter initialization will be deferred until the first forward\n",
        "            datasets pass at which time the size will be determined.\n",
        "        out_channels (int): Number of channels of output arrays.\n",
        "        ksize (int or pair of ints): Size of filters (a.k.a. kernels).\n",
        "            ``ksize=k`` and ``ksize=(k, k)`` are equivalent.\n",
        "        stride (int or pair of ints): Stride of filter applications.\n",
        "            ``stride=s`` and ``stride=(s, s)`` are equivalent.\n",
        "        pad (int or pair of ints): Spatial padding width for input arrays.\n",
        "            ``pad=p`` and ``pad=(p, p)`` are equivalent.\n",
        "        wscale (float): Scaling factor of the initial weight.\n",
        "        bias (float): Initial bias value.\n",
        "        nobias (bool): If ``True``, then this link does not use the bias term.\n",
        "        initialW (4-D array): Initial weight value. If ``None``, then this\n",
        "            function uses to initialize ``wscale``.\n",
        "            May also be a callable that takes ``numpy.ndarray`` or\n",
        "            ``cupy.ndarray`` and edits its value.\n",
        "        initial_bias (1-D array): Initial bias value. If ``None``, then this\n",
        "            function uses to initialize ``bias``.\n",
        "            May also be a callable that takes ``numpy.ndarray`` or\n",
        "            ``cupy.ndarray`` and edits its value.\n",
        "        use_gamma (bool): If true, apply scalar multiplication to the \n",
        "            normalized weight (i.e. reparameterize).\n",
        "        Ip (int): The number of power iteration for calculating the spcetral \n",
        "            norm of the weights.\n",
        "        factor (float) : constant factor to adjust spectral norm of W_bar.\n",
        "    .. seealso::\n",
        "       See :func:`chainer.functions.convolution_2d` for the definition of\n",
        "       two-dimensional convolution.\n",
        "    Attributes:\n",
        "        W (~chainer.Variable): Weight parameter.\n",
        "        W_bar (~chainer.Variable): Spectrally normalized weight parameter.\n",
        "        b (~chainer.Variable): Bias parameter.\n",
        "        u (~numpy.array): Current estimation of the right largest singular vector of W.\n",
        "        (optional) gamma (~chainer.Variable): the multiplier parameter.\n",
        "        (optional) factor (float): constant factor to adjust spectral norm of W_bar.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, ksize, stride=1, pad=0,\n",
        "                 nobias=False, initialW=None, initial_bias=None, use_gamma=False, Ip=1, factor=None):\n",
        "        self.Ip = Ip\n",
        "        self.use_gamma = use_gamma\n",
        "        self.factor = factor\n",
        "        super(SNConvolution2D, self).__init__(\n",
        "            in_channels, out_channels, ksize, stride, pad,\n",
        "            nobias, initialW, initial_bias)\n",
        "        self.u = np.random.normal(size=(1, out_channels)).astype(dtype=\"f\")\n",
        "        self.register_persistent('u')\n",
        "\n",
        "    @property\n",
        "    def W_bar(self):\n",
        "        \"\"\"\n",
        "        Spectrally Normalized Weight\n",
        "        \"\"\"\n",
        "        W_mat = self.W.reshape(self.W.shape[0], -1)\n",
        "        sigma, _u, _ = max_singular_value(W_mat, self.u, self.Ip)\n",
        "        if self.factor:\n",
        "            sigma = sigma / self.factor\n",
        "        sigma = broadcast_to(sigma.reshape((1, 1, 1, 1)), self.W.shape)\n",
        "        if chainer.config.train:\n",
        "            # Update estimated 1st singular vector\n",
        "            self.u[:] = _u\n",
        "        if hasattr(self, 'gamma'):\n",
        "            return broadcast_to(self.gamma, self.W.shape) * self.W / sigma\n",
        "        else:\n",
        "            return self.W / sigma\n",
        "\n",
        "    def _initialize_params(self, in_size):\n",
        "        super(SNConvolution2D, self)._initialize_params(in_size)\n",
        "        if self.use_gamma:\n",
        "            W_mat = self.W.data.reshape(self.W.shape[0], -1)\n",
        "            _, s, _ = np.linalg.svd(W_mat)\n",
        "            with self.init_scope():\n",
        "                self.gamma = chainer.Parameter(s[0], (1, 1, 1, 1))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Applies the convolution layer.\n",
        "        Args:\n",
        "            x (~chainer.Variable): Input image.\n",
        "        Returns:\n",
        "            ~chainer.Variable: Output of the convolution.\n",
        "        \"\"\"\n",
        "        if self.W.data is None:\n",
        "            self._initialize_params(x.shape[1])\n",
        "        return convolution_2d.convolution_2d(\n",
        "            x, self.W_bar, self.b, self.stride, self.pad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM4w8vMKS8in",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _downsample(x):\n",
        "    # Downsample (Mean Avg Pooling with 2x2 kernel)\n",
        "    return F.average_pooling_2d(x, 2)\n",
        "\n",
        "\n",
        "class DBlock(chainer.Chain):\n",
        "    def __init__(self, in_channels, out_channels, hidden_channels=None, ksize=3, pad=1,\n",
        "                 activation=F.relu, downsample=False):\n",
        "        super(DBlock, self).__init__()\n",
        "        initializer = chainer.initializers.GlorotUniform(math.sqrt(2))\n",
        "        initializer_sc = chainer.initializers.GlorotUniform()\n",
        "        self.activation = activation\n",
        "        self.downsample = downsample\n",
        "        self.learnable_sc = (in_channels != out_channels) or downsample\n",
        "        hidden_channels = in_channels if hidden_channels is None else hidden_channels\n",
        "        with self.init_scope():\n",
        "            self.c1 = SNConvolution2D(in_channels, hidden_channels, ksize=ksize, pad=pad, initialW=initializer)\n",
        "            self.c2 = SNConvolution2D(hidden_channels, out_channels, ksize=ksize, pad=pad, initialW=initializer)\n",
        "            if self.learnable_sc:\n",
        "                self.c_sc = SNConvolution2D(in_channels, out_channels, ksize=1, pad=0, initialW=initializer_sc)\n",
        "\n",
        "    def residual(self, x):\n",
        "        h = x\n",
        "        h = self.activation(h)\n",
        "        h = self.c1(h)\n",
        "        h = self.activation(h)\n",
        "        h = self.c2(h)\n",
        "        if self.downsample:\n",
        "            h = _downsample(h)\n",
        "        return h\n",
        "\n",
        "    def shortcut(self, x):\n",
        "        if self.learnable_sc:\n",
        "            x = self.c_sc(x)\n",
        "            if self.downsample:\n",
        "                return _downsample(x)\n",
        "            else:\n",
        "                return x\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.residual(x) + self.shortcut(x)\n",
        "\n",
        "\n",
        "class OptimizedBlock(chainer.Chain):\n",
        "    def __init__(self, in_channels, out_channels, ksize=3, pad=1, activation=F.relu):\n",
        "        super(OptimizedBlock, self).__init__()\n",
        "        initializer = chainer.initializers.GlorotUniform(math.sqrt(2))\n",
        "        initializer_sc = chainer.initializers.GlorotUniform()\n",
        "        self.activation = activation\n",
        "        with self.init_scope():\n",
        "            self.c1 = SNConvolution2D(in_channels, out_channels, ksize=ksize, pad=pad, initialW=initializer)\n",
        "            self.c2 = SNConvolution2D(out_channels, out_channels, ksize=ksize, pad=pad, initialW=initializer)\n",
        "            self.c_sc = SNConvolution2D(in_channels, out_channels, ksize=1, pad=0, initialW=initializer_sc)\n",
        "\n",
        "    def residual(self, x):\n",
        "        h = x\n",
        "        h = self.c1(h)\n",
        "        h = self.activation(h)\n",
        "        h = self.c2(h)\n",
        "        h = _downsample(h)\n",
        "        return h\n",
        "\n",
        "    def shortcut(self, x):\n",
        "        return self.c_sc(_downsample(x))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.residual(x) + self.shortcut(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lagog6cdQNzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_noise(h, sigma=0.2):\n",
        "    xp = cuda.get_array_module(h.data)\n",
        "    if chainer.config.train:\n",
        "        return h + sigma * xp.random.randn(*h.shape)\n",
        "    else:\n",
        "        return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY-831NFQMJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(chainer.Chain):\n",
        "\n",
        "    def __init__(self, bottom_width=4, bottom_height=4, ch=128, n_classes=10, wscale=0.02, activation=F.relu):\n",
        "        w = chainer.initializers.GlorotUniform()\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.activation = activation\n",
        "        with self.init_scope():\n",
        "            self.block1 = OptimizedBlock(3, ch)\n",
        "            self.block2 = DBlock(ch, ch, activation=activation, downsample=True)\n",
        "            self.block3 = DBlock(ch, ch, activation=activation, downsample=False)\n",
        "            self.block4 = DBlock(ch, ch, activation=activation, downsample=False)\n",
        "            self.block5 = DBlock(ch * 4, ch * 8, activation=activation, downsample=True)\n",
        "            self.block6 = DBlock(ch * 8, ch * 8, activation=activation, downsample=False)\n",
        "            self.l7 = SNLinear(ch, 1, initialW=w, nobias=True)\n",
        "            if n_classes > 0:\n",
        "                self.l_y = SNEmbedID(n_classes, ch, initialW=w)\n",
        "\n",
        "    def __call__(self, x, y):\n",
        "        h = x\n",
        "        h = self.block1(h)\n",
        "        h = self.block2(h)\n",
        "        h = self.block3(h)\n",
        "        h = self.block4(h)\n",
        "\n",
        "        #h = self.block5(h)\n",
        "        #h = self.block6(h)\n",
        "        h = self.activation(h)\n",
        "        \n",
        "        h = F.sum(h, axis=(2, 3))  # Global pooling\n",
        "        \n",
        "        output = self.l7(h)\n",
        "        \n",
        "        if y is not None:\n",
        "            w_y = self.l_y(y)\n",
        "            output += F.sum(w_y * h, axis=1, keepdims=True)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMBOImQjZeaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen = Generator(n_hidden=n_hidden)\n",
        "dis = Discriminator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaZR2PeVZjcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup an optimizer\n",
        "def make_optimizer(model, alpha=0.0002, beta1=0.0, beta2=0.9):\n",
        "    optimizer = chainer.optimizers.Adam(alpha=alpha, beta1=beta1, beta2=beta2)\n",
        "    optimizer.setup(model)\n",
        "    return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNKJ1wPRQPZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt_gen = make_optimizer(gen)\n",
        "opt_dis = make_optimizer(dis)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjEb6qZrQ-uG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_hinge_dis(dis_fake, dis_real):\n",
        "    loss = F.mean(F.relu(1. - dis_real))\n",
        "    loss += F.mean(F.relu(1. + dis_fake))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def loss_hinge_gen(dis_fake):\n",
        "    loss = -F.mean(dis_fake)\n",
        "    return loss\n",
        "\n",
        "\n",
        "class DCGANUpdater(chainer.training.updaters.StandardUpdater):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.gen, self.dis = kwargs.pop('models')\n",
        "        super(DCGANUpdater, self).__init__(*args, **kwargs)\n",
        "        self.xp = self.gen.xp\n",
        "        self.loss_gen = loss_hinge_gen\n",
        "        self.loss_dis = loss_hinge_dis\n",
        "\n",
        "    def loss_dis(self, dis, y_fake, y_real):\n",
        "        batchsize = len(y_fake)\n",
        "        L1 = F.sum(F.softplus(-y_real)) /  batchsize\n",
        "        L2 = F.sum(F.softplus(y_fake)) /  batchsize\n",
        "        \n",
        "        loss = L1 + L2\n",
        "        chainer.report({'loss': loss}, dis)\n",
        "        return loss\n",
        "\n",
        "    def loss_gen(self, gen, y_fake):\n",
        "        batchsize = len(y_fake)\n",
        "        loss = F.sum(F.softplus(-y_fake)) /  batchsize\n",
        "        chainer.report({'loss': loss}, gen)\n",
        "        return loss\n",
        "    \n",
        "    def loss_hinge_dis(self, dis, dis_fake, dis_real):\n",
        "        batchsize = len(dis_fake)\n",
        "        loss = F.mean(F.relu(1. - dis_real))\n",
        "        loss += F.mean(F.relu(1. + dis_fake))\n",
        "        chainer.report({'loss': loss}, dis)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def loss_hinge_gen(self, gen, dis_fake):\n",
        "        batchsize = len(dis_fake)\n",
        "        loss = -F.mean(dis_fake)\n",
        "        chainer.report({'loss': loss}, gen)\n",
        "        return loss\n",
        "\n",
        "    def make_onehot(self, classes):\n",
        "        return self.xp.eye(10, dtype='f')[classes]\n",
        "\n",
        "    def update_core(self):\n",
        "        gen_optimizer = self.get_optimizer('gen')\n",
        "        dis_optimizer = self.get_optimizer('dis')\n",
        "        gen, dis = self.gen, self.dis\n",
        "        \n",
        "        for i in range(5):\n",
        "          if i == 0:\n",
        "            z = Variable(self.xp.asarray(gen.make_hidden(128)))\n",
        "            onehot = self.xp.random.randint(low=0, high=10, size=(128)).astype(self.xp.int32)\n",
        "            x_fake = gen(z, onehot)\n",
        "            y_fake = dis(x_fake, onehot)\n",
        "            loss_gen = self.loss_gen(dis_fake=y_fake)\n",
        "            gen.cleargrads()\n",
        "            loss_gen.backward()\n",
        "            gen_optimizer.update()\n",
        "            chainer.reporter.report({'loss_gen': loss_gen})\n",
        "            \n",
        "          batch = self.get_iterator('main').next()\n",
        "          batchsize = len(batch)\n",
        "          images = [batch[i][0] for i in range(batchsize)]\n",
        "          labels = [batch[i][1] for i in range(batchsize)]\n",
        "        #test = np.array(images).repeat(2, axis=2)\n",
        "        #test = np.array(test).repeat(2, axis=3)\n",
        "        \n",
        "          x_real = Variable(self.converter(images, self.device)) / 255.\n",
        "        \n",
        "          x_label = Variable(self.converter(labels, self.device))\n",
        "        \n",
        "          xp_label = chainer.cuda.get_array_module(x_label.data)\n",
        "          labels = xp_label.asarray(labels)\n",
        "        \n",
        "          classes = self.xp.random.random_integers(0, 9, len(batch))\n",
        "          #onehot = chainer.Variable(self.make_onehot(classes))\n",
        "          classes = chainer.Variable(classes)\n",
        "        \n",
        "          xp = chainer.backends.cuda.get_array_module(x_real.data)\n",
        "        \n",
        "         \n",
        "        \n",
        "        \n",
        "          y_real = dis(x_real, labels)\n",
        "          x_fake = gen(z, onehot)\n",
        "          y_fake = dis(x_fake, onehot)\n",
        "          x_fake.unchain_backward()\n",
        "\n",
        "          loss_dis = self.loss_dis(dis_fake=y_fake, dis_real=y_real)\n",
        "          dis.cleargrads()\n",
        "          loss_dis.backward()\n",
        "          dis_optimizer.update()\n",
        "          chainer.reporter.report({'loss_dis': loss_dis})\n",
        "        \n",
        "          \n",
        "          \n",
        "\n",
        "          #dis_optimizer.update(self.loss_dis, dis, y_fake, y_real)\n",
        "          #gen_optimizer.update(self.loss_gen, gen, y_fake)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--UyAQApQWtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import chainer.backends.cuda\n",
        "\n",
        "def out_generated_image(gen, dis, rows, cols, seed, dst):\n",
        "    @chainer.training.make_extension()\n",
        "    def make_image(trainer):\n",
        "        np.random.seed(seed)\n",
        "        n_images = rows * cols\n",
        "        xp = gen.xp\n",
        "        classes = np.arange(0, 10, dtype=np.int32)\n",
        "        for c in classes:\n",
        "          for n_c in range(10):\n",
        "            #y = xp.asarray([c] * n_images, dtype=xp.int32)\n",
        "            ys = xp.array([[c, n_c]] * 10, dtype=xp.int32)\n",
        "            z = Variable(xp.asarray(gen.make_hidden(n_images)))\n",
        "            #z = np.array([np.random.normal(size=(128,))] * 10, np.float32)\n",
        "            ws_y = xp.array([np.linspace(0, 1, 10)[::-1], np.linspace(0, 1, 10)], dtype=xp.float32).T\n",
        "            with chainer.using_config('train', False):\n",
        "                x = gen(z, ys, weights=ws_y)\n",
        "            x = chainer.backends.cuda.to_cpu(x.data)\n",
        "            np.random.seed()\n",
        "\n",
        "            x = np.asarray(np.clip(x * 255.0, 0.0, 255.0), dtype=np.uint8)\n",
        "            _, _, H, W = x.shape\n",
        "            x = x.reshape((rows, cols, 3, H, W))\n",
        "            x = x.transpose(0, 3, 1, 4, 2)\n",
        "            x = x.reshape((rows * H, cols * W, 3))\n",
        "\n",
        "            preview_dir = '{}/preview'.format(dst)\n",
        "            preview_path = preview_dir +\\\n",
        "                '/image{}{}{:0>8}.png'.format(c, n_c, trainer.updater.iteration)\n",
        "            if not os.path.exists(preview_dir):\n",
        "                os.makedirs(preview_dir)\n",
        "            Image.fromarray(x).save(preview_path)\n",
        "    return make_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbJnF8rKcxx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "updater = DCGANUpdater(\n",
        "      models=(gen, dis),\n",
        "      iterator=train_iter,\n",
        "      optimizer={\n",
        "          'gen': opt_gen, 'dis': opt_dis},\n",
        "      device=gpu_id)\n",
        "trainer = chainer.training.Trainer(updater, (n_epoch, 'epoch'), out=out_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddKM13U7QdFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "snapshot_interval = (snapshot_interval, 'iteration')\n",
        "display_interval = (display_interval, 'iteration')\n",
        "trainer.extend(\n",
        "    extensions.snapshot(filename='snapshot_iter_{.updater.iteration}.npz'),\n",
        "    trigger=snapshot_interval)\n",
        "trainer.extend(extensions.snapshot_object(\n",
        "    gen, 'gen_iter_{.updater.iteration}.npz'), trigger=snapshot_interval)\n",
        "trainer.extend(extensions.snapshot_object(\n",
        "    dis, 'dis_iter_{.updater.iteration}.npz'), trigger=snapshot_interval)\n",
        "trainer.extend(extensions.LogReport(trigger=display_interval))\n",
        "trainer.extend(extensions.PrintReport([\n",
        "    'epoch', 'iteration', 'loss_dis', 'loss_gen',\n",
        "]), trigger=display_interval)\n",
        "trainer.extend(extensions.ProgressBar(update_interval=100))\n",
        "trainer.extend(\n",
        "    out_generated_image(\n",
        "        gen, dis,\n",
        "        1, 10, seed, out_dir),\n",
        "    trigger=snapshot_interval)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOgCeR7tQlx9",
        "colab_type": "code",
        "outputId": "a402785c-d552-4e47-b5f1-13b2d601177b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13294
        }
      },
      "source": [
        "# Run the training\n",
        "trainer.run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch       iteration   loss_dis    loss_gen  \n",
            "\u001b[J0           100         0.257759    2.7263      \n",
            "\u001b[J     total [..................................................]  0.64%\n",
            "this epoch [################################..................] 64.00%\n",
            "       100 iter, 0 epoch / 100 epochs\n",
            "       inf iters/sec. Estimated time to finish: 0:00:00.\n",
            "\u001b[4A\u001b[J1           200         0.470142    1.92009     \n",
            "\u001b[J     total [..................................................]  1.28%\n",
            "this epoch [##############....................................] 28.00%\n",
            "       200 iter, 1 epoch / 100 epochs\n",
            "   0.72915 iters/sec. Estimated time to finish: 5:52:34.803033.\n",
            "\u001b[4A\u001b[J1           300         0.800062    1.49027     \n",
            "\u001b[J     total [..................................................]  1.92%\n",
            "this epoch [##############################################....] 92.00%\n",
            "       300 iter, 1 epoch / 100 epochs\n",
            "   0.72858 iters/sec. Estimated time to finish: 5:50:34.002925.\n",
            "\u001b[4A\u001b[J2           400         1.02801     1.15298     \n",
            "\u001b[J     total [#.................................................]  2.56%\n",
            "this epoch [############################......................] 56.00%\n",
            "       400 iter, 2 epoch / 100 epochs\n",
            "   0.72842 iters/sec. Estimated time to finish: 5:48:21.473924.\n",
            "\u001b[4A\u001b[J3           500         1.21531     0.973334    \n",
            "\u001b[J     total [#.................................................]  3.20%\n",
            "this epoch [##########........................................] 20.00%\n",
            "       500 iter, 3 epoch / 100 epochs\n",
            "   0.72809 iters/sec. Estimated time to finish: 5:46:13.576688.\n",
            "\u001b[4A\u001b[J3           600         1.31777     0.71625     \n",
            "\u001b[J     total [#.................................................]  3.84%\n",
            "this epoch [#########################################.........] 84.00%\n",
            "       600 iter, 3 epoch / 100 epochs\n",
            "   0.72762 iters/sec. Estimated time to finish: 5:44:09.387388.\n",
            "\u001b[4A\u001b[J4           700         1.32411     0.635469    \n",
            "\u001b[J     total [##................................................]  4.48%\n",
            "this epoch [########################..........................] 48.00%\n",
            "       700 iter, 4 epoch / 100 epochs\n",
            "   0.72748 iters/sec. Estimated time to finish: 5:41:56.133413.\n",
            "\u001b[4A\u001b[J5           800         1.35785     0.586817    \n",
            "\u001b[J     total [##................................................]  5.12%\n",
            "this epoch [######............................................] 12.00%\n",
            "       800 iter, 5 epoch / 100 epochs\n",
            "   0.72731 iters/sec. Estimated time to finish: 5:39:43.376635.\n",
            "\u001b[4A\u001b[J5           900         1.40975     0.660178    \n",
            "\u001b[J     total [##................................................]  5.76%\n",
            "this epoch [#####################################.............] 76.00%\n",
            "       900 iter, 5 epoch / 100 epochs\n",
            "     0.727 iters/sec. Estimated time to finish: 5:37:34.402628.\n",
            "\u001b[4A\u001b[J6           1000        1.45574     0.368948    \n",
            "\u001b[J     total [###...............................................]  6.40%\n",
            "this epoch [####################..............................] 40.00%\n",
            "      1000 iter, 6 epoch / 100 epochs\n",
            "     0.727 iters/sec. Estimated time to finish: 5:35:16.995655.\n",
            "\u001b[4A\u001b[J7           1100        1.45594     0.289955    \n",
            "\u001b[J     total [###...............................................]  7.04%\n",
            "this epoch [##................................................]  4.00%\n",
            "      1100 iter, 7 epoch / 100 epochs\n",
            "   0.71269 iters/sec. Estimated time to finish: 5:39:40.588818.\n",
            "\u001b[4A\u001b[J7           1200        1.43886     0.387029    \n",
            "\u001b[J     total [###...............................................]  7.68%\n",
            "this epoch [#################################.................] 68.00%\n",
            "      1200 iter, 7 epoch / 100 epochs\n",
            "   0.71388 iters/sec. Estimated time to finish: 5:36:46.406952.\n",
            "\u001b[4A\u001b[J8           1300        1.48609     0.503985    \n",
            "\u001b[J     total [####..............................................]  8.32%\n",
            "this epoch [################..................................] 32.00%\n",
            "      1300 iter, 8 epoch / 100 epochs\n",
            "   0.71473 iters/sec. Estimated time to finish: 5:34:02.631594.\n",
            "\u001b[4A\u001b[J8           1400        1.49953     0.328731    \n",
            "\u001b[J     total [####..............................................]  8.96%\n",
            "this epoch [################################################..] 96.00%\n",
            "      1400 iter, 8 epoch / 100 epochs\n",
            "   0.71547 iters/sec. Estimated time to finish: 5:31:21.924159.\n",
            "\u001b[4A\u001b[J9           1500        1.49073     0.367682    \n",
            "\u001b[J     total [####..............................................]  9.60%\n",
            "this epoch [#############################.....................] 60.00%\n",
            "      1500 iter, 9 epoch / 100 epochs\n",
            "   0.71604 iters/sec. Estimated time to finish: 5:28:46.428352.\n",
            "\u001b[4A\u001b[J10          1600        1.5432      0.387997    \n",
            "\u001b[J     total [#####.............................................] 10.24%\n",
            "this epoch [############......................................] 24.00%\n",
            "      1600 iter, 10 epoch / 100 epochs\n",
            "   0.71645 iters/sec. Estimated time to finish: 5:26:15.707844.\n",
            "\u001b[4A\u001b[J10          1700        1.52772     0.514496    \n",
            "\u001b[J     total [#####.............................................] 10.88%\n",
            "this epoch [############################################......] 88.00%\n",
            "      1700 iter, 10 epoch / 100 epochs\n",
            "   0.71702 iters/sec. Estimated time to finish: 5:23:40.652608.\n",
            "\u001b[4A\u001b[J11          1800        1.48752     0.408405    \n",
            "\u001b[J     total [#####.............................................] 11.52%\n",
            "this epoch [#########################.........................] 52.00%\n",
            "      1800 iter, 11 epoch / 100 epochs\n",
            "   0.71758 iters/sec. Estimated time to finish: 5:21:06.120457.\n",
            "\u001b[4A\u001b[J12          1900        1.55653     0.307077    \n",
            "\u001b[J     total [######............................................] 12.16%\n",
            "this epoch [########..........................................] 16.00%\n",
            "      1900 iter, 12 epoch / 100 epochs\n",
            "   0.71807 iters/sec. Estimated time to finish: 5:18:33.630473.\n",
            "\u001b[4A\u001b[J12          2000        1.53578     0.284101    \n",
            "\u001b[J     total [######............................................] 12.80%\n",
            "this epoch [########################################..........] 80.00%\n",
            "      2000 iter, 12 epoch / 100 epochs\n",
            "   0.71849 iters/sec. Estimated time to finish: 5:16:03.504911.\n",
            "\u001b[4A\u001b[J13          2100        1.56463     0.393701    \n",
            "\u001b[J     total [######............................................] 13.44%\n",
            "this epoch [#####################.............................] 44.00%\n",
            "      2100 iter, 13 epoch / 100 epochs\n",
            "   0.71259 iters/sec. Estimated time to finish: 5:16:20.180092.\n",
            "\u001b[4A\u001b[J14          2200        1.55551     0.415603    \n",
            "\u001b[J     total [#######...........................................] 14.08%\n",
            "this epoch [####..............................................]  8.00%\n",
            "      2200 iter, 14 epoch / 100 epochs\n",
            "   0.71317 iters/sec. Estimated time to finish: 5:13:44.455922.\n",
            "\u001b[4A\u001b[J14          2300        1.58824     0.478597    \n",
            "\u001b[J     total [#######...........................................] 14.72%\n",
            "this epoch [####################################..............] 72.00%\n",
            "      2300 iter, 14 epoch / 100 epochs\n",
            "   0.71376 iters/sec. Estimated time to finish: 5:11:08.864926.\n",
            "\u001b[4A\u001b[J15          2400        1.56287     0.367594    \n",
            "\u001b[J     total [#######...........................................] 15.36%\n",
            "this epoch [#################.................................] 36.00%\n",
            "      2400 iter, 15 epoch / 100 epochs\n",
            "   0.71427 iters/sec. Estimated time to finish: 5:08:35.319498.\n",
            "\u001b[4A\u001b[J16          2500        1.56933     0.236658    \n",
            "\u001b[J     total [########..........................................] 16.00%\n",
            "this epoch [..................................................]  0.00%\n",
            "      2500 iter, 16 epoch / 100 epochs\n",
            "   0.71479 iters/sec. Estimated time to finish: 5:06:01.938290.\n",
            "\u001b[4A\u001b[J16          2600        1.55537     0.270292    \n",
            "\u001b[J     total [########..........................................] 16.64%\n",
            "this epoch [################################..................] 64.00%\n",
            "      2600 iter, 16 epoch / 100 epochs\n",
            "   0.71523 iters/sec. Estimated time to finish: 5:03:30.922451.\n",
            "\u001b[4A\u001b[J17          2700        1.56879     0.239957    \n",
            "\u001b[J     total [########..........................................] 17.28%\n",
            "this epoch [##############....................................] 28.00%\n",
            "      2700 iter, 17 epoch / 100 epochs\n",
            "   0.71561 iters/sec. Estimated time to finish: 5:01:01.408171.\n",
            "\u001b[4A\u001b[J17          2800        1.59433     0.381657    \n",
            "\u001b[J     total [########..........................................] 17.92%\n",
            "this epoch [##############################################....] 92.00%\n",
            "      2800 iter, 17 epoch / 100 epochs\n",
            "   0.71584 iters/sec. Estimated time to finish: 4:58:35.976064.\n",
            "\u001b[4A\u001b[J18          2900        1.59979     0.349685    \n",
            "\u001b[J     total [#########.........................................] 18.56%\n",
            "this epoch [###########################.......................] 56.00%\n",
            "      2900 iter, 18 epoch / 100 epochs\n",
            "   0.71611 iters/sec. Estimated time to finish: 4:56:09.625475.\n",
            "\u001b[4A\u001b[J19          3000        1.57435     0.340055    \n",
            "\u001b[J     total [#########.........................................] 19.20%\n",
            "this epoch [#########.........................................] 20.00%\n",
            "      3000 iter, 19 epoch / 100 epochs\n",
            "   0.71633 iters/sec. Estimated time to finish: 4:53:44.615291.\n",
            "\u001b[4A\u001b[J19          3100        1.58254     0.262146    \n",
            "\u001b[J     total [#########.........................................] 19.84%\n",
            "this epoch [#########################################.........] 84.00%\n",
            "      3100 iter, 19 epoch / 100 epochs\n",
            "   0.71211 iters/sec. Estimated time to finish: 4:53:08.537061.\n",
            "\u001b[4A\u001b[J20          3200        1.58592     0.240331    \n",
            "\u001b[J     total [##########........................................] 20.48%\n",
            "this epoch [########################..........................] 48.00%\n",
            "      3200 iter, 20 epoch / 100 epochs\n",
            "   0.71244 iters/sec. Estimated time to finish: 4:50:40.146263.\n",
            "\u001b[4A\u001b[J21          3300        1.59903     0.314538    \n",
            "\u001b[J     total [##########........................................] 21.12%\n",
            "this epoch [######............................................] 12.00%\n",
            "      3300 iter, 21 epoch / 100 epochs\n",
            "   0.71276 iters/sec. Estimated time to finish: 4:48:11.865626.\n",
            "\u001b[4A\u001b[J21          3400        1.60366     0.210676    \n",
            "\u001b[J     total [##########........................................] 21.76%\n",
            "this epoch [######################################............] 76.00%\n",
            "      3400 iter, 21 epoch / 100 epochs\n",
            "   0.71305 iters/sec. Estimated time to finish: 4:45:44.612212.\n",
            "\u001b[4A\u001b[J22          3500        1.6388      0.318863    \n",
            "\u001b[J     total [###########.......................................] 22.40%\n",
            "this epoch [###################...............................] 40.00%\n",
            "      3500 iter, 22 epoch / 100 epochs\n",
            "   0.71329 iters/sec. Estimated time to finish: 4:43:18.668320.\n",
            "\u001b[4A\u001b[J23          3600        1.58859     0.312634    \n",
            "\u001b[J     total [###########.......................................] 23.04%\n",
            "this epoch [#.................................................]  4.00%\n",
            "      3600 iter, 23 epoch / 100 epochs\n",
            "   0.71354 iters/sec. Estimated time to finish: 4:40:52.595288.\n",
            "\u001b[4A\u001b[J23          3700        1.61778     0.206281    \n",
            "\u001b[J     total [###########.......................................] 23.68%\n",
            "this epoch [#################################.................] 68.00%\n",
            "      3700 iter, 23 epoch / 100 epochs\n",
            "   0.71376 iters/sec. Estimated time to finish: 4:38:27.181874.\n",
            "\u001b[4A\u001b[J24          3800        1.57091     0.299931    \n",
            "\u001b[J     total [############......................................] 24.32%\n",
            "this epoch [################..................................] 32.00%\n",
            "      3800 iter, 24 epoch / 100 epochs\n",
            "   0.71399 iters/sec. Estimated time to finish: 4:36:01.918489.\n",
            "\u001b[4A\u001b[J24          3900        1.59536     0.23083     \n",
            "\u001b[J     total [############......................................] 24.96%\n",
            "this epoch [################################################..] 96.00%\n",
            "      3900 iter, 24 epoch / 100 epochs\n",
            "   0.71421 iters/sec. Estimated time to finish: 4:33:36.656190.\n",
            "\u001b[4A\u001b[J25          4000        1.63384     0.226716    \n",
            "\u001b[J     total [############......................................] 25.60%\n",
            "this epoch [##############################....................] 60.00%\n",
            "      4000 iter, 25 epoch / 100 epochs\n",
            "   0.71441 iters/sec. Estimated time to finish: 4:31:12.063877.\n",
            "\u001b[4A\u001b[J26          4100        1.62404     0.325771    \n",
            "\u001b[J     total [#############.....................................] 26.24%\n",
            "this epoch [###########.......................................] 24.00%\n",
            "      4100 iter, 26 epoch / 100 epochs\n",
            "   0.71135 iters/sec. Estimated time to finish: 4:30:01.570188.\n",
            "\u001b[4A\u001b[J26          4200        1.62461     0.220933    \n",
            "\u001b[J     total [#############.....................................] 26.88%\n",
            "this epoch [###########################################.......] 88.00%\n",
            "      4200 iter, 26 epoch / 100 epochs\n",
            "   0.71162 iters/sec. Estimated time to finish: 4:27:34.899696.\n",
            "\u001b[4A\u001b[J27          4300        1.6297      0.134926    \n",
            "\u001b[J     total [#############.....................................] 27.52%\n",
            "this epoch [#########################.........................] 52.00%\n",
            "      4300 iter, 27 epoch / 100 epochs\n",
            "   0.71188 iters/sec. Estimated time to finish: 4:25:08.602681.\n",
            "\u001b[4A\u001b[J28          4400        1.60464     0.249064    \n",
            "\u001b[J     total [##############....................................] 28.16%\n",
            "this epoch [########..........................................] 16.00%\n",
            "      4400 iter, 28 epoch / 100 epochs\n",
            "   0.71212 iters/sec. Estimated time to finish: 4:22:42.819307.\n",
            "\u001b[4A\u001b[J28          4500        1.6262      0.197763    \n",
            "\u001b[J     total [##############....................................] 28.80%\n",
            "this epoch [########################################..........] 80.00%\n",
            "      4500 iter, 28 epoch / 100 epochs\n",
            "   0.71235 iters/sec. Estimated time to finish: 4:20:17.297391.\n",
            "\u001b[4A\u001b[J29          4600        1.63547     0.211462    \n",
            "\u001b[J     total [##############....................................] 29.44%\n",
            "this epoch [######################............................] 44.00%\n",
            "      4600 iter, 29 epoch / 100 epochs\n",
            "   0.71258 iters/sec. Estimated time to finish: 4:17:51.879574.\n",
            "\u001b[4A\u001b[J30          4700        1.62722     0.232357    \n",
            "\u001b[J     total [###############...................................] 30.08%\n",
            "this epoch [###...............................................]  8.00%\n",
            "      4700 iter, 30 epoch / 100 epochs\n",
            "    0.7128 iters/sec. Estimated time to finish: 4:15:26.926979.\n",
            "\u001b[4A\u001b[J30          4800        1.63153     0.209334    \n",
            "\u001b[J     total [###############...................................] 30.72%\n",
            "this epoch [###################################...............] 72.00%\n",
            "      4800 iter, 30 epoch / 100 epochs\n",
            "   0.71298 iters/sec. Estimated time to finish: 4:13:02.658117.\n",
            "\u001b[4A\u001b[J31          4900        1.65425     0.208859    \n",
            "\u001b[J     total [###############...................................] 31.36%\n",
            "this epoch [#################.................................] 36.00%\n",
            "      4900 iter, 31 epoch / 100 epochs\n",
            "   0.71319 iters/sec. Estimated time to finish: 4:10:38.041814.\n",
            "\u001b[4A\u001b[J32          5000        1.66124     0.149421    \n",
            "\u001b[J     total [################..................................] 32.00%\n",
            "this epoch [..................................................]  0.00%\n",
            "      5000 iter, 32 epoch / 100 epochs\n",
            "   0.71337 iters/sec. Estimated time to finish: 4:08:14.134986.\n",
            "\u001b[4A\u001b[J32          5100        1.62826     0.272523    \n",
            "\u001b[J     total [################..................................] 32.64%\n",
            "this epoch [################################..................] 64.00%\n",
            "      5100 iter, 32 epoch / 100 epochs\n",
            "   0.71077 iters/sec. Estimated time to finish: 4:06:47.964312.\n",
            "\u001b[4A\u001b[J33          5200        1.65911     0.24837     \n",
            "\u001b[J     total [################..................................] 33.28%\n",
            "this epoch [##############....................................] 28.00%\n",
            "      5200 iter, 33 epoch / 100 epochs\n",
            "     0.711 iters/sec. Estimated time to finish: 4:04:22.353953.\n",
            "\u001b[4A\u001b[J33          5300        1.64685     0.144578    \n",
            "\u001b[J     total [################..................................] 33.92%\n",
            "this epoch [##############################################....] 92.00%\n",
            "      5300 iter, 33 epoch / 100 epochs\n",
            "   0.71123 iters/sec. Estimated time to finish: 4:01:57.030090.\n",
            "\u001b[4A\u001b[J34          5400        1.68368     0.256815    \n",
            "\u001b[J     total [#################.................................] 34.56%\n",
            "this epoch [############################......................] 56.00%\n",
            "      5400 iter, 34 epoch / 100 epochs\n",
            "   0.71143 iters/sec. Estimated time to finish: 3:59:32.446414.\n",
            "\u001b[4A\u001b[J35          5500        1.65676     0.160785    \n",
            "\u001b[J     total [#################.................................] 35.20%\n",
            "this epoch [##########........................................] 20.00%\n",
            "      5500 iter, 35 epoch / 100 epochs\n",
            "   0.71165 iters/sec. Estimated time to finish: 3:57:07.492357.\n",
            "\u001b[4A\u001b[J35          5600        1.67355     0.256332    \n",
            "\u001b[J     total [#################.................................] 35.84%\n",
            "this epoch [##########################################........] 84.00%\n",
            "      5600 iter, 35 epoch / 100 epochs\n",
            "   0.71185 iters/sec. Estimated time to finish: 3:54:43.041474.\n",
            "\u001b[4A\u001b[J36          5700        1.66209     0.235958    \n",
            "\u001b[J     total [##################................................] 36.48%\n",
            "this epoch [#######################...........................] 48.00%\n",
            "      5700 iter, 36 epoch / 100 epochs\n",
            "   0.71203 iters/sec. Estimated time to finish: 3:52:19.037961.\n",
            "\u001b[4A\u001b[J37          5800        1.68411     0.191554    \n",
            "\u001b[J     total [##################................................] 37.12%\n",
            "this epoch [#####.............................................] 12.00%\n",
            "      5800 iter, 37 epoch / 100 epochs\n",
            "    0.7122 iters/sec. Estimated time to finish: 3:49:55.233817.\n",
            "\u001b[4A\u001b[J37          5900        1.66171     0.0730781   \n",
            "\u001b[J     total [##################................................] 37.76%\n",
            "this epoch [#####################################.............] 76.00%\n",
            "      5900 iter, 37 epoch / 100 epochs\n",
            "   0.71238 iters/sec. Estimated time to finish: 3:47:31.359798.\n",
            "\u001b[4A\u001b[J38          6000        1.66756     0.177442    \n",
            "\u001b[J     total [###################...............................] 38.40%\n",
            "this epoch [###################...............................] 40.00%\n",
            "      6000 iter, 38 epoch / 100 epochs\n",
            "   0.71255 iters/sec. Estimated time to finish: 3:45:07.777615.\n",
            "\u001b[4A\u001b[J39          6100        1.67161     0.18366     \n",
            "\u001b[J     total [###################...............................] 39.04%\n",
            "this epoch [#.................................................]  4.00%\n",
            "      6100 iter, 39 epoch / 100 epochs\n",
            "   0.71054 iters/sec. Estimated time to finish: 3:43:25.233522.\n",
            "\u001b[4A\u001b[J39          6200        1.65701     0.117445    \n",
            "\u001b[J     total [###################...............................] 39.68%\n",
            "this epoch [#################################.................] 68.00%\n",
            "      6200 iter, 39 epoch / 100 epochs\n",
            "   0.71074 iters/sec. Estimated time to finish: 3:41:00.795846.\n",
            "\u001b[4A\u001b[J40          6300        1.73566     0.169023    \n",
            "\u001b[J     total [####################..............................] 40.32%\n",
            "this epoch [################..................................] 32.00%\n",
            "      6300 iter, 40 epoch / 100 epochs\n",
            "   0.71092 iters/sec. Estimated time to finish: 3:38:36.872590.\n",
            "\u001b[4A\u001b[J40          6400        1.67034     0.288597    \n",
            "\u001b[J     total [####################..............................] 40.96%\n",
            "this epoch [################################################..] 96.00%\n",
            "      6400 iter, 40 epoch / 100 epochs\n",
            "   0.71109 iters/sec. Estimated time to finish: 3:36:13.086229.\n",
            "\u001b[4A\u001b[J41          6500        1.67915     0.170987    \n",
            "\u001b[J     total [####################..............................] 41.60%\n",
            "this epoch [##############################....................] 60.00%\n",
            "      6500 iter, 41 epoch / 100 epochs\n",
            "   0.71125 iters/sec. Estimated time to finish: 3:33:49.454925.\n",
            "\u001b[4A\u001b[J42          6600        1.66669     0.186597    \n",
            "\u001b[J     total [#####################.............................] 42.24%\n",
            "this epoch [############......................................] 24.00%\n",
            "      6600 iter, 42 epoch / 100 epochs\n",
            "   0.71141 iters/sec. Estimated time to finish: 3:31:26.066496.\n",
            "\u001b[4A\u001b[J42          6700        1.67018     0.171004    \n",
            "\u001b[J     total [#####################.............................] 42.88%\n",
            "this epoch [############################################......] 88.00%\n",
            "      6700 iter, 42 epoch / 100 epochs\n",
            "    0.7116 iters/sec. Estimated time to finish: 3:29:02.243588.\n",
            "\u001b[4A\u001b[J43          6800        1.6878      0.172215    \n",
            "\u001b[J     total [#####################.............................] 43.52%\n",
            "this epoch [##########################........................] 52.00%\n",
            "      6800 iter, 43 epoch / 100 epochs\n",
            "   0.71174 iters/sec. Estimated time to finish: 3:26:39.195403.\n",
            "\u001b[4A\u001b[J44          6900        1.65902     0.210077    \n",
            "\u001b[J     total [######################............................] 44.16%\n",
            "this epoch [#######...........................................] 16.00%\n",
            "      6900 iter, 44 epoch / 100 epochs\n",
            "    0.7119 iters/sec. Estimated time to finish: 3:24:15.962982.\n",
            "\u001b[4A\u001b[J44          7000        1.6962      0.184385    \n",
            "\u001b[J     total [######################............................] 44.80%\n",
            "this epoch [#######################################...........] 80.00%\n",
            "      7000 iter, 44 epoch / 100 epochs\n",
            "   0.71207 iters/sec. Estimated time to finish: 3:21:52.615255.\n",
            "\u001b[4A\u001b[J45          7100        1.69779     0.0711588   \n",
            "\u001b[J     total [######################............................] 45.44%\n",
            "this epoch [#####################.............................] 44.00%\n",
            "      7100 iter, 45 epoch / 100 epochs\n",
            "   0.71038 iters/sec. Estimated time to finish: 3:20:00.545840.\n",
            "\u001b[4A\u001b[J46          7200        1.69282     0.163701    \n",
            "\u001b[J     total [#######################...........................] 46.08%\n",
            "this epoch [###...............................................]  8.00%\n",
            "      7200 iter, 46 epoch / 100 epochs\n",
            "   0.71066 iters/sec. Estimated time to finish: 3:17:35.227283.\n",
            "\u001b[4A\u001b[J46          7300        1.68762     0.208875    \n",
            "\u001b[J     total [#######################...........................] 46.72%\n",
            "this epoch [###################################...............] 72.00%\n",
            "      7300 iter, 46 epoch / 100 epochs\n",
            "   0.71093 iters/sec. Estimated time to finish: 3:15:09.990965.\n",
            "\u001b[4A\u001b[J47          7400        1.68318     0.182748    \n",
            "\u001b[J     total [#######################...........................] 47.36%\n",
            "this epoch [#################.................................] 36.00%\n",
            "      7400 iter, 47 epoch / 100 epochs\n",
            "   0.71119 iters/sec. Estimated time to finish: 3:12:45.105699.\n",
            "\u001b[4A\u001b[J48          7500        1.6864      0.150349    \n",
            "\u001b[J     total [########################..........................] 48.00%\n",
            "this epoch [..................................................]  0.00%\n",
            "      7500 iter, 48 epoch / 100 epochs\n",
            "   0.71144 iters/sec. Estimated time to finish: 3:10:20.428257.\n",
            "\u001b[4A\u001b[J48          7600        1.69513     0.207723    \n",
            "\u001b[J     total [########################..........................] 48.64%\n",
            "this epoch [################################..................] 64.00%\n",
            "      7600 iter, 48 epoch / 100 epochs\n",
            "   0.71169 iters/sec. Estimated time to finish: 3:07:55.911964.\n",
            "\u001b[4A\u001b[J49          7700        1.73265     0.21182     \n",
            "\u001b[J     total [########################..........................] 49.28%\n",
            "this epoch [##############....................................] 28.00%\n",
            "      7700 iter, 49 epoch / 100 epochs\n",
            "   0.71192 iters/sec. Estimated time to finish: 3:05:31.844055.\n",
            "\u001b[4A\u001b[J49          7800        1.68395     0.128629    \n",
            "\u001b[J     total [########################..........................] 49.92%\n",
            "this epoch [##############################################....] 92.00%\n",
            "      7800 iter, 49 epoch / 100 epochs\n",
            "   0.71216 iters/sec. Estimated time to finish: 3:03:07.757748.\n",
            "\u001b[4A\u001b[J50          7900        1.70701     0.182997    \n",
            "\u001b[J     total [#########################.........................] 50.56%\n",
            "this epoch [############################......................] 56.00%\n",
            "      7900 iter, 50 epoch / 100 epochs\n",
            "   0.71238 iters/sec. Estimated time to finish: 3:00:43.892154.\n",
            "\u001b[4A\u001b[J51          8000        1.69279     0.179839    \n",
            "\u001b[J     total [#########################.........................] 51.20%\n",
            "this epoch [##########........................................] 20.00%\n",
            "      8000 iter, 51 epoch / 100 epochs\n",
            "   0.71261 iters/sec. Estimated time to finish: 2:58:20.060225.\n",
            "\u001b[4A\u001b[J51          8100        1.72046     0.285487    \n",
            "\u001b[J     total [#########################.........................] 51.84%\n",
            "this epoch [##########################################........] 84.00%\n",
            "      8100 iter, 51 epoch / 100 epochs\n",
            "   0.71119 iters/sec. Estimated time to finish: 2:56:20.920848.\n",
            "\u001b[4A\u001b[J52          8200        1.7091      0.103309    \n",
            "\u001b[J     total [##########################........................] 52.48%\n",
            "this epoch [#######################...........................] 48.00%\n",
            "      8200 iter, 52 epoch / 100 epochs\n",
            "   0.71142 iters/sec. Estimated time to finish: 2:53:56.828500.\n",
            "\u001b[4A\u001b[J53          8300        1.69602     0.167949    \n",
            "\u001b[J     total [##########################........................] 53.12%\n",
            "this epoch [#####.............................................] 12.00%\n",
            "      8300 iter, 53 epoch / 100 epochs\n",
            "   0.71163 iters/sec. Estimated time to finish: 2:51:33.288058.\n",
            "\u001b[4A\u001b[J53          8400        1.69356     0.241321    \n",
            "\u001b[J     total [##########################........................] 53.76%\n",
            "this epoch [#####################################.............] 76.00%\n",
            "      8400 iter, 53 epoch / 100 epochs\n",
            "   0.71185 iters/sec. Estimated time to finish: 2:49:09.616727.\n",
            "\u001b[4A\u001b[J54          8500        1.68528     0.119427    \n",
            "\u001b[J     total [###########################.......................] 54.40%\n",
            "this epoch [###################...............................] 40.00%\n",
            "      8500 iter, 54 epoch / 100 epochs\n",
            "   0.71206 iters/sec. Estimated time to finish: 2:46:46.241840.\n",
            "\u001b[4A\u001b[J55          8600        1.71479     0.189221    \n",
            "\u001b[J     total [###########################.......................] 55.04%\n",
            "this epoch [#.................................................]  4.00%\n",
            "      8600 iter, 55 epoch / 100 epochs\n",
            "   0.71226 iters/sec. Estimated time to finish: 2:44:22.999607.\n",
            "\u001b[4A\u001b[J55          8700        1.69683     0.247598    \n",
            "\u001b[J     total [###########################.......................] 55.68%\n",
            "this epoch [#################################.................] 68.00%\n",
            "      8700 iter, 55 epoch / 100 epochs\n",
            "   0.71246 iters/sec. Estimated time to finish: 2:41:59.804933.\n",
            "\u001b[4A\u001b[J56          8800        1.7237      0.119478    \n",
            "\u001b[J     total [############################......................] 56.32%\n",
            "this epoch [################..................................] 32.00%\n",
            "      8800 iter, 56 epoch / 100 epochs\n",
            "   0.71265 iters/sec. Estimated time to finish: 2:39:36.940284.\n",
            "\u001b[4A\u001b[J56          8900        1.73329     0.200889    \n",
            "\u001b[J     total [############################......................] 56.96%\n",
            "this epoch [################################################..] 96.00%\n",
            "      8900 iter, 56 epoch / 100 epochs\n",
            "   0.71287 iters/sec. Estimated time to finish: 2:37:13.689383.\n",
            "\u001b[4A\u001b[J57          9000        1.70654     0.0699835   \n",
            "\u001b[J     total [############################......................] 57.60%\n",
            "this epoch [##############################....................] 60.00%\n",
            "      9000 iter, 57 epoch / 100 epochs\n",
            "    0.7131 iters/sec. Estimated time to finish: 2:34:50.477682.\n",
            "\u001b[4A\u001b[J58          9100        1.72356     0.195765    \n",
            "\u001b[J     total [#############################.....................] 58.24%\n",
            "this epoch [############......................................] 24.00%\n",
            "      9100 iter, 58 epoch / 100 epochs\n",
            "   0.71183 iters/sec. Estimated time to finish: 2:32:46.501476.\n",
            "\u001b[4A\u001b[J58          9200        1.70527     0.229671    \n",
            "\u001b[J     total [#############################.....................] 58.88%\n",
            "this epoch [############################################......] 88.00%\n",
            "      9200 iter, 58 epoch / 100 epochs\n",
            "   0.71199 iters/sec. Estimated time to finish: 2:30:24.050842.\n",
            "\u001b[4A\u001b[J59          9300        1.7198      0.0736091   \n",
            "\u001b[J     total [#############################.....................] 59.52%\n",
            "this epoch [##########################........................] 52.00%\n",
            "      9300 iter, 59 epoch / 100 epochs\n",
            "   0.71215 iters/sec. Estimated time to finish: 2:28:01.586238.\n",
            "\u001b[4A\u001b[J60          9400        1.7323      0.201489    \n",
            "\u001b[J     total [##############################....................] 60.16%\n",
            "this epoch [#######...........................................] 16.00%\n",
            "      9400 iter, 60 epoch / 100 epochs\n",
            "   0.71229 iters/sec. Estimated time to finish: 2:25:39.458830.\n",
            "\u001b[4A\u001b[J60          9500        1.71642     0.0966013   \n",
            "\u001b[J     total [##############################....................] 60.80%\n",
            "this epoch [#######################################...........] 80.00%\n",
            "      9500 iter, 60 epoch / 100 epochs\n",
            "   0.71243 iters/sec. Estimated time to finish: 2:23:17.309759.\n",
            "\u001b[4A\u001b[J61          9600        1.72308     0.137845    \n",
            "\u001b[J     total [##############################....................] 61.44%\n",
            "this epoch [#####################.............................] 44.00%\n",
            "      9600 iter, 61 epoch / 100 epochs\n",
            "   0.71258 iters/sec. Estimated time to finish: 2:20:55.237999.\n",
            "\u001b[4A\u001b[J62          9700        1.70161     0.163236    \n",
            "\u001b[J     total [###############################...................] 62.08%\n",
            "this epoch [###...............................................]  8.00%\n",
            "      9700 iter, 62 epoch / 100 epochs\n",
            "   0.71271 iters/sec. Estimated time to finish: 2:18:33.333920.\n",
            "\u001b[4A\u001b[J62          9800        1.72516     0.0052914   \n",
            "\u001b[J     total [###############################...................] 62.72%\n",
            "this epoch [###################################...............] 72.00%\n",
            "      9800 iter, 62 epoch / 100 epochs\n",
            "   0.71285 iters/sec. Estimated time to finish: 2:16:11.463468.\n",
            "\u001b[4A\u001b[J63          9900        1.7386      0.206839    \n",
            "\u001b[J     total [###############################...................] 63.36%\n",
            "this epoch [#################.................................] 36.00%\n",
            "      9900 iter, 63 epoch / 100 epochs\n",
            "   0.71298 iters/sec. Estimated time to finish: 2:13:49.627846.\n",
            "\u001b[4A\u001b[J64          10000       1.71167     0.0787358   \n",
            "\u001b[J     total [################################..................] 64.00%\n",
            "this epoch [..................................................]  0.00%\n",
            "     10000 iter, 64 epoch / 100 epochs\n",
            "   0.71312 iters/sec. Estimated time to finish: 2:11:27.886490.\n",
            "\u001b[4A\u001b[J64          10100       1.73087     0.193207    \n",
            "\u001b[J     total [################################..................] 64.64%\n",
            "this epoch [################################..................] 64.00%\n",
            "     10100 iter, 64 epoch / 100 epochs\n",
            "   0.71196 iters/sec. Estimated time to finish: 2:09:20.310698.\n",
            "\u001b[4A\u001b[J65          10200       1.73279     0.0560731   \n",
            "\u001b[J     total [################################..................] 65.28%\n",
            "this epoch [##############....................................] 28.00%\n",
            "     10200 iter, 65 epoch / 100 epochs\n",
            "   0.71199 iters/sec. Estimated time to finish: 2:06:59.454648.\n",
            "\u001b[4A\u001b[J65          10300       1.73436     0.0757995   \n",
            "\u001b[J     total [################################..................] 65.92%\n",
            "this epoch [##############################################....] 92.00%\n",
            "     10300 iter, 65 epoch / 100 epochs\n",
            "   0.71203 iters/sec. Estimated time to finish: 2:04:38.635064.\n",
            "\u001b[4A\u001b[J66          10400       1.72599     0.12253     \n",
            "\u001b[J     total [#################################.................] 66.56%\n",
            "this epoch [############################......................] 56.00%\n",
            "     10400 iter, 66 epoch / 100 epochs\n",
            "   0.71209 iters/sec. Estimated time to finish: 2:02:17.584048.\n",
            "\u001b[4A\u001b[J67          10500       1.71074     0.164013    \n",
            "\u001b[J     total [#################################.................] 67.20%\n",
            "this epoch [##########........................................] 20.00%\n",
            "     10500 iter, 67 epoch / 100 epochs\n",
            "   0.71214 iters/sec. Estimated time to finish: 1:59:56.621000.\n",
            "\u001b[4A\u001b[J67          10600       1.71945     0.194249    \n",
            "\u001b[J     total [#################################.................] 67.84%\n",
            "this epoch [##########################################........] 84.00%\n",
            "     10600 iter, 67 epoch / 100 epochs\n",
            "   0.71221 iters/sec. Estimated time to finish: 1:57:35.526112.\n",
            "\u001b[4A\u001b[J68          10700       1.72473     0.193157    \n",
            "\u001b[J     total [##################################................] 68.48%\n",
            "this epoch [########################..........................] 48.00%\n",
            "     10700 iter, 68 epoch / 100 epochs\n",
            "   0.71228 iters/sec. Estimated time to finish: 1:55:14.452216.\n",
            "\u001b[4A\u001b[J69          10800       1.72934     0.0530707   \n",
            "\u001b[J     total [##################################................] 69.12%\n",
            "this epoch [######............................................] 12.00%\n",
            "     10800 iter, 69 epoch / 100 epochs\n",
            "   0.71235 iters/sec. Estimated time to finish: 1:52:53.353689.\n",
            "\u001b[4A\u001b[J69          10900       1.73901     0.171597    \n",
            "\u001b[J     total [##################################................] 69.76%\n",
            "this epoch [######################################............] 76.00%\n",
            "     10900 iter, 69 epoch / 100 epochs\n",
            "   0.71242 iters/sec. Estimated time to finish: 1:50:32.288895.\n",
            "\u001b[4A\u001b[J70          11000       1.74617     0.204308    \n",
            "\u001b[J     total [###################################...............] 70.40%\n",
            "this epoch [####################..............................] 40.00%\n",
            "     11000 iter, 70 epoch / 100 epochs\n",
            "   0.71247 iters/sec. Estimated time to finish: 1:48:11.465299.\n",
            "\u001b[4A\u001b[J71          11100       1.74109     0.110459    \n",
            "\u001b[J     total [###################################...............] 71.04%\n",
            "this epoch [##................................................]  4.00%\n",
            "     11100 iter, 71 epoch / 100 epochs\n",
            "   0.71267 iters/sec. Estimated time to finish: 1:45:49.376537.\n",
            "\u001b[4A\u001b[J71          11200       1.76462     0.196511    \n",
            "\u001b[J     total [###################################...............] 71.68%\n",
            "this epoch [##################################................] 68.00%\n",
            "     11200 iter, 71 epoch / 100 epochs\n",
            "   0.71273 iters/sec. Estimated time to finish: 1:43:28.485381.\n",
            "\u001b[4A\u001b[J72          11300       1.75392     0.0823403   \n",
            "\u001b[J     total [####################################..............] 72.32%\n",
            "this epoch [###############...................................] 32.00%\n",
            "     11300 iter, 72 epoch / 100 epochs\n",
            "   0.71284 iters/sec. Estimated time to finish: 1:41:07.296337.\n",
            "\u001b[4A\u001b[J72          11400       1.72238     0.053494    \n",
            "\u001b[J     total [####################################..............] 72.96%\n",
            "this epoch [###############################################...] 96.00%\n",
            "     11400 iter, 72 epoch / 100 epochs\n",
            "   0.71292 iters/sec. Estimated time to finish: 1:38:46.315492.\n",
            "\u001b[4A\u001b[J73          11500       1.73744     0.0429182   \n",
            "\u001b[J     total [####################################..............] 73.60%\n",
            "this epoch [#############################.....................] 60.00%\n",
            "     11500 iter, 73 epoch / 100 epochs\n",
            "   0.71302 iters/sec. Estimated time to finish: 1:36:25.263832.\n",
            "\u001b[4A\u001b[J74          11600       1.72949     0.127771    \n",
            "\u001b[J     total [#####################################.............] 74.24%\n",
            "this epoch [###########.......................................] 24.00%\n",
            "     11600 iter, 74 epoch / 100 epochs\n",
            "   0.71314 iters/sec. Estimated time to finish: 1:34:04.086694.\n",
            "\u001b[4A\u001b[J74          11700       1.75198     0.0639088   \n",
            "\u001b[J     total [#####################################.............] 74.88%\n",
            "this epoch [###########################################.......] 88.00%\n",
            "     11700 iter, 74 epoch / 100 epochs\n",
            "   0.71321 iters/sec. Estimated time to finish: 1:31:43.304922.\n",
            "\u001b[4A\u001b[J75          11800       1.76585     -0.0167277  \n",
            "\u001b[J     total [#####################################.............] 75.52%\n",
            "this epoch [#########################.........................] 52.00%\n",
            "     11800 iter, 75 epoch / 100 epochs\n",
            "   0.71326 iters/sec. Estimated time to finish: 1:29:22.694464.\n",
            "\u001b[4A\u001b[J76          11900       1.76419     0.0623397   \n",
            "\u001b[J     total [######################################............] 76.16%\n",
            "this epoch [#######...........................................] 16.00%\n",
            "     11900 iter, 76 epoch / 100 epochs\n",
            "   0.71333 iters/sec. Estimated time to finish: 1:27:02.019588.\n",
            "\u001b[4A\u001b[J76          12000       1.75842     0.163749    \n",
            "\u001b[J     total [######################################............] 76.80%\n",
            "this epoch [#######################################...........] 80.00%\n",
            "     12000 iter, 76 epoch / 100 epochs\n",
            "    0.7134 iters/sec. Estimated time to finish: 1:24:41.275367.\n",
            "\u001b[4A\u001b[J77          12100       1.76679     0.1233      \n",
            "\u001b[J     total [######################################............] 77.44%\n",
            "this epoch [#####################.............................] 44.00%\n",
            "     12100 iter, 77 epoch / 100 epochs\n",
            "   0.71337 iters/sec. Estimated time to finish: 1:22:21.317715.\n",
            "\u001b[4A\u001b[J78          12200       1.76502     0.119682    \n",
            "\u001b[J     total [#######################################...........] 78.08%\n",
            "this epoch [###...............................................]  8.00%\n",
            "     12200 iter, 78 epoch / 100 epochs\n",
            "   0.71339 iters/sec. Estimated time to finish: 1:20:01.044608.\n",
            "\u001b[4A\u001b[J78          12300       1.74075     0.0421726   \n",
            "\u001b[J     total [#######################################...........] 78.72%\n",
            "this epoch [###################################...............] 72.00%\n",
            "     12300 iter, 78 epoch / 100 epochs\n",
            "   0.71338 iters/sec. Estimated time to finish: 1:17:40.940730.\n",
            "\u001b[4A\u001b[J79          12400       1.77315     0.145568    \n",
            "\u001b[J     total [#######################################...........] 79.36%\n",
            "this epoch [#################.................................] 36.00%\n",
            "     12400 iter, 79 epoch / 100 epochs\n",
            "   0.71337 iters/sec. Estimated time to finish: 1:15:20.765269.\n",
            "\u001b[4A\u001b[J80          12500       1.75855     0.130035    \n",
            "\u001b[J     total [########################################..........] 80.00%\n",
            "this epoch [..................................................]  0.00%\n",
            "     12500 iter, 80 epoch / 100 epochs\n",
            "   0.71337 iters/sec. Estimated time to finish: 1:13:00.639672.\n",
            "\u001b[4A\u001b[J80          12600       1.77052     0.0873104   \n",
            "\u001b[J     total [########################################..........] 80.64%\n",
            "this epoch [################################..................] 64.00%\n",
            "     12600 iter, 80 epoch / 100 epochs\n",
            "   0.71338 iters/sec. Estimated time to finish: 1:10:40.369333.\n",
            "\u001b[4A\u001b[J81          12700       1.76578     0.187197    \n",
            "\u001b[J     total [########################################..........] 81.28%\n",
            "this epoch [##############....................................] 28.00%\n",
            "     12700 iter, 81 epoch / 100 epochs\n",
            "   0.71339 iters/sec. Estimated time to finish: 1:08:20.144065.\n",
            "\u001b[4A\u001b[J81          12800       1.76232     0.111814    \n",
            "\u001b[J     total [########################################..........] 81.92%\n",
            "this epoch [##############################################....] 92.00%\n",
            "     12800 iter, 81 epoch / 100 epochs\n",
            "   0.71344 iters/sec. Estimated time to finish: 1:05:59.706258.\n",
            "\u001b[4A\u001b[J82          12900       1.77871     0.134251    \n",
            "\u001b[J     total [#########################################.........] 82.56%\n",
            "this epoch [############################......................] 56.00%\n",
            "     12900 iter, 82 epoch / 100 epochs\n",
            "   0.71347 iters/sec. Estimated time to finish: 1:03:39.381296.\n",
            "\u001b[4A\u001b[J83          13000       1.773       0.10746     \n",
            "\u001b[J     total [#########################################.........] 83.20%\n",
            "this epoch [##########........................................] 20.00%\n",
            "     13000 iter, 83 epoch / 100 epochs\n",
            "   0.71349 iters/sec. Estimated time to finish: 1:01:19.091314.\n",
            "\u001b[4A\u001b[J83          13100       1.77843     0.0765368   \n",
            "\u001b[J     total [#########################################.........] 83.84%\n",
            "this epoch [##########################################........] 84.00%\n",
            "     13100 iter, 83 epoch / 100 epochs\n",
            "   0.71356 iters/sec. Estimated time to finish: 0:58:58.583383.\n",
            "\u001b[4A\u001b[J84          13200       1.75306     0.0615357   \n",
            "\u001b[J     total [##########################################........] 84.48%\n",
            "this epoch [########################..........................] 48.00%\n",
            "     13200 iter, 84 epoch / 100 epochs\n",
            "   0.71368 iters/sec. Estimated time to finish: 0:56:37.904247.\n",
            "\u001b[4A\u001b[J85          13300       1.76803     0.0904186   \n",
            "\u001b[J     total [##########################################........] 85.12%\n",
            "this epoch [######............................................] 12.00%\n",
            "     13300 iter, 85 epoch / 100 epochs\n",
            "   0.71377 iters/sec. Estimated time to finish: 0:54:17.345261.\n",
            "\u001b[4A\u001b[J85          13400       1.78369     0.119168    \n",
            "\u001b[J     total [##########################################........] 85.76%\n",
            "this epoch [######################################............] 76.00%\n",
            "     13400 iter, 85 epoch / 100 epochs\n",
            "   0.71388 iters/sec. Estimated time to finish: 0:51:56.761983.\n",
            "\u001b[4A\u001b[J86          13500       1.78875     -0.0138375  \n",
            "\u001b[J     total [###########################################.......] 86.40%\n",
            "this epoch [####################..............................] 40.00%\n",
            "     13500 iter, 86 epoch / 100 epochs\n",
            "   0.71399 iters/sec. Estimated time to finish: 0:49:36.228710.\n",
            "\u001b[4A\u001b[J87          13600       1.77885     0.071335    \n",
            "\u001b[J     total [###########################################.......] 87.04%\n",
            "this epoch [##................................................]  4.00%\n",
            "     13600 iter, 87 epoch / 100 epochs\n",
            "   0.71409 iters/sec. Estimated time to finish: 0:47:15.781574.\n",
            "\u001b[4A\u001b[J87          13700       1.77202     0.0496016   \n",
            "\u001b[J     total [###########################################.......] 87.68%\n",
            "this epoch [##################################................] 68.00%\n",
            "     13700 iter, 87 epoch / 100 epochs\n",
            "   0.71418 iters/sec. Estimated time to finish: 0:44:55.383249.\n",
            "\u001b[4A\u001b[J88          13800       1.77132     0.147132    \n",
            "\u001b[J     total [############################################......] 88.32%\n",
            "this epoch [###############...................................] 32.00%\n",
            "     13800 iter, 88 epoch / 100 epochs\n",
            "   0.71428 iters/sec. Estimated time to finish: 0:42:35.023475.\n",
            "\u001b[4A\u001b[J88          13900       1.77952     0.118475    \n",
            "\u001b[J     total [############################################......] 88.96%\n",
            "this epoch [###############################################...] 96.00%\n",
            "     13900 iter, 88 epoch / 100 epochs\n",
            "   0.71438 iters/sec. Estimated time to finish: 0:40:14.681460.\n",
            "\u001b[4A\u001b[J89          14000       1.76935     0.139225    \n",
            "\u001b[J     total [############################################......] 89.60%\n",
            "this epoch [#############################.....................] 60.00%\n",
            "     14000 iter, 89 epoch / 100 epochs\n",
            "   0.71449 iters/sec. Estimated time to finish: 0:37:54.345952.\n",
            "\u001b[4A\u001b[J90          14100       1.77482     0.188373    \n",
            "\u001b[J     total [#############################################.....] 90.24%\n",
            "this epoch [###########.......................................] 24.00%\n",
            "     14100 iter, 90 epoch / 100 epochs\n",
            "   0.71462 iters/sec. Estimated time to finish: 0:35:33.993608.\n",
            "\u001b[4A\u001b[J90          14200       1.7809      0.0569941   \n",
            "\u001b[J     total [#############################################.....] 90.88%\n",
            "this epoch [###########################################.......] 88.00%\n",
            "     14200 iter, 90 epoch / 100 epochs\n",
            "   0.71473 iters/sec. Estimated time to finish: 0:33:13.748486.\n",
            "\u001b[4A\u001b[J91          14300       1.78454     0.129       \n",
            "\u001b[J     total [#############################################.....] 91.52%\n",
            "this epoch [#########################.........................] 52.00%\n",
            "     14300 iter, 91 epoch / 100 epochs\n",
            "   0.71484 iters/sec. Estimated time to finish: 0:30:53.566669.\n",
            "\u001b[4A\u001b[J92          14400       1.77724     0.0571495   \n",
            "\u001b[J     total [##############################################....] 92.16%\n",
            "this epoch [#######...........................................] 16.00%\n",
            "     14400 iter, 92 epoch / 100 epochs\n",
            "   0.71495 iters/sec. Estimated time to finish: 0:28:33.409227.\n",
            "\u001b[4A\u001b[J92          14500       1.79992     0.120127    \n",
            "\u001b[J     total [##############################################....] 92.80%\n",
            "this epoch [#######################################...........] 80.00%\n",
            "     14500 iter, 92 epoch / 100 epochs\n",
            "   0.71503 iters/sec. Estimated time to finish: 0:26:13.369175.\n",
            "\u001b[4A\u001b[J93          14600       1.79875     0.0270232   \n",
            "\u001b[J     total [##############################################....] 93.44%\n",
            "this epoch [#####################.............................] 44.00%\n",
            "     14600 iter, 93 epoch / 100 epochs\n",
            "    0.7151 iters/sec. Estimated time to finish: 0:23:53.357071.\n",
            "\u001b[4A\u001b[J94          14700       1.78936     0.118344    \n",
            "\u001b[J     total [###############################################...] 94.08%\n",
            "this epoch [###...............................................]  8.00%\n",
            "     14700 iter, 94 epoch / 100 epochs\n",
            "   0.71519 iters/sec. Estimated time to finish: 0:21:33.369445.\n",
            "\u001b[4A\u001b[J94          14800       1.78088     0.0759839   \n",
            "\u001b[J     total [###############################################...] 94.72%\n",
            "this epoch [###################################...............] 72.00%\n",
            "     14800 iter, 94 epoch / 100 epochs\n",
            "   0.71529 iters/sec. Estimated time to finish: 0:19:13.383606.\n",
            "\u001b[4A\u001b[J95          14900       1.79693     0.214974    \n",
            "\u001b[J     total [###############################################...] 95.36%\n",
            "this epoch [#################.................................] 36.00%\n",
            "     14900 iter, 95 epoch / 100 epochs\n",
            "   0.71537 iters/sec. Estimated time to finish: 0:16:53.461409.\n",
            "\u001b[4A\u001b[J96          15000       1.79796     0.0765198   \n",
            "\u001b[J     total [################################################..] 96.00%\n",
            "this epoch [..................................................]  0.00%\n",
            "     15000 iter, 96 epoch / 100 epochs\n",
            "   0.71547 iters/sec. Estimated time to finish: 0:14:33.546501.\n",
            "\u001b[4A\u001b[J96          15100       1.78716     0.122118    \n",
            "\u001b[J     total [################################################..] 96.64%\n",
            "this epoch [################################..................] 64.00%\n",
            "     15100 iter, 96 epoch / 100 epochs\n",
            "   0.71562 iters/sec. Estimated time to finish: 0:12:13.634351.\n",
            "\u001b[4A\u001b[J97          15200       1.79809     0.12078     \n",
            "\u001b[J     total [################################################..] 97.28%\n",
            "this epoch [##############....................................] 28.00%\n",
            "     15200 iter, 97 epoch / 100 epochs\n",
            "   0.71565 iters/sec. Estimated time to finish: 0:09:53.868878.\n",
            "\u001b[4A\u001b[J97          15300       1.80611     0.0955373   \n",
            "\u001b[J     total [################################################..] 97.92%\n",
            "this epoch [##############################################....] 92.00%\n",
            "     15300 iter, 97 epoch / 100 epochs\n",
            "   0.71568 iters/sec. Estimated time to finish: 0:07:34.114661.\n",
            "\u001b[4A\u001b[J98          15400       1.81259     0.118639    \n",
            "\u001b[J     total [#################################################.] 98.56%\n",
            "this epoch [############################......................] 56.00%\n",
            "     15400 iter, 98 epoch / 100 epochs\n",
            "   0.71572 iters/sec. Estimated time to finish: 0:05:14.368502.\n",
            "\u001b[4A\u001b[J99          15500       1.79092     0.0601471   \n",
            "\u001b[J     total [#################################################.] 99.20%\n",
            "this epoch [##########........................................] 20.00%\n",
            "     15500 iter, 99 epoch / 100 epochs\n",
            "   0.71575 iters/sec. Estimated time to finish: 0:02:54.642254.\n",
            "\u001b[4A\u001b[J99          15600       1.80768     0.1275      \n",
            "\u001b[J     total [#################################################.] 99.84%\n",
            "this epoch [##########################################........] 84.00%\n",
            "     15600 iter, 99 epoch / 100 epochs\n",
            "    0.7158 iters/sec. Estimated time to finish: 0:00:34.926093.\n",
            "\u001b[4A\u001b[J"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Whbljy_pwfpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image, display_png\n",
        "import glob\n",
        "\n",
        "image_files = sorted(glob.glob(out_dir + '/preview/*.png'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hMW0qYmwkVw",
        "colab_type": "code",
        "outputId": "d682cba4-5de5-4833-95cb-f75dae5eb6ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "display_png(Image(image_files[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAAgCAIAAADscTYTAABNQUlEQVR4nH39WbBtWXYdho0551pr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\n"
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3PWjcUjwljh",
        "colab_type": "code",
        "outputId": "c7269f45-271e-40f7-9d27-c6198c43f1a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "display_png(Image(image_files[-1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAAgCAIAAADscTYTAABVH0lEQVR4nJX9abBsWXYehq219t5n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\nT0fUrGUAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}